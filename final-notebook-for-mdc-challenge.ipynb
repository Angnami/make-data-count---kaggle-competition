{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "from functools import partial\n",
    "import collections\n",
    "from collections import Counter , defaultdict\n",
    "import random\n",
    "from typing import List, Optional, Dict, Tuple, Union \n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import re \n",
    "import difflib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET    \n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "                            AutoTokenizer, AutoModelForTokenClassification,                \n",
    "                            AutoModelForSequenceClassification, Trainer,                    \n",
    "                            TrainingArguments, PreTrainedTokenizer, \n",
    "                            DataCollatorForTokenClassification, \n",
    "                            DataCollatorWithPadding,PreTrainedTokenizer, \n",
    "                            PreTrainedModel,EarlyStoppingCallback\n",
    "                        )\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pymupdf\n",
    "import seqeval\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score,accuracy_score\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD THE DATA FROM KAGGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the data from [make-data-count-finding-data-references](https://www.kaggle.com/competitions/make-data-count-finding-data-references/data). Extract the data into a folder called ```data```.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A WORKING DIRECTORY AND ALL NECESSARY FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.787707Z",
     "iopub.status.busy": "2025-09-09T18:52:30.787396Z",
     "iopub.status.idle": "2025-09-09T18:52:30.802761Z",
     "shell.execute_reply": "2025-09-09T18:52:30.801823Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.787676Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = \"./\" # You can customize\n",
    "WORKING_DIR = Path(f\"{ROOT_PATH}\")\n",
    "WORKING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_FILE_PATH = WORKING_DIR/\"submission.csv\"\n",
    "DATA_ROOT = WORKING_DIR/\"data\"\n",
    "TRAIN_PDF_FILES = DATA_ROOT/ \"train/PDF/\"\n",
    "TRAIN_XML_FILES = DATA_ROOT/\"train/XML/\"\n",
    "TRAIN_LABELS_PATH = DATA_ROOT/\"train_labels.csv\"\n",
    "TEST_PDF_FILES = DATA_ROOT/ \"test/PDF/\"\n",
    "TEST_XML_FILES = DATA_ROOT/\"test/XML/\"\n",
    "MODELS_DIR = WORKING_DIR/\"Models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.804119Z",
     "iopub.status.busy": "2025-09-09T18:52:30.803816Z",
     "iopub.status.idle": "2025-09-09T18:52:30.824395Z",
     "shell.execute_reply": "2025-09-09T18:52:30.823322Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.804094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher toutes les lignes\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Afficher toutes les colonnes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.828512Z",
     "iopub.status.busy": "2025-09-09T18:52:30.828149Z",
     "iopub.status.idle": "2025-09-09T18:52:30.89169Z",
     "shell.execute_reply": "2025-09-09T18:52:30.890842Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.828483Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "train_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.893171Z",
     "iopub.status.busy": "2025-09-09T18:52:30.892826Z",
     "iopub.status.idle": "2025-09-09T18:52:30.904678Z",
     "shell.execute_reply": "2025-09-09T18:52:30.903837Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.89314Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels_df.article_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.906094Z",
     "iopub.status.busy": "2025-09-09T18:52:30.90581Z",
     "iopub.status.idle": "2025-09-09T18:52:30.921908Z",
     "shell.execute_reply": "2025-09-09T18:52:30.921025Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.906073Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.92302Z",
     "iopub.status.busy": "2025-09-09T18:52:30.922701Z",
     "iopub.status.idle": "2025-09-09T18:52:30.953209Z",
     "shell.execute_reply": "2025-09-09T18:52:30.95214Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.923001Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels_df.groupby(\"article_id\").agg('count').sort_values(by=\"dataset_id\",ascending=False)[\"dataset_id\"][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.95465Z",
     "iopub.status.busy": "2025-09-09T18:52:30.954268Z",
     "iopub.status.idle": "2025-09-09T18:52:30.98976Z",
     "shell.execute_reply": "2025-09-09T18:52:30.988894Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.954621Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pdf_articles = [art.stem  for art in TRAIN_PDF_FILES.glob(\"*.pdf\")]\n",
    "train_xml_articles = [art.stem  for art in TRAIN_XML_FILES.glob(\"*.xml\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.990921Z",
     "iopub.status.busy": "2025-09-09T18:52:30.99066Z",
     "iopub.status.idle": "2025-09-09T18:52:30.996347Z",
     "shell.execute_reply": "2025-09-09T18:52:30.995578Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.9909Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_pdf_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:30.997597Z",
     "iopub.status.busy": "2025-09-09T18:52:30.997302Z",
     "iopub.status.idle": "2025-09-09T18:52:31.015639Z",
     "shell.execute_reply": "2025-09-09T18:52:31.01471Z",
     "shell.execute_reply.started": "2025-09-09T18:52:30.997578Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_xml_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.017186Z",
     "iopub.status.busy": "2025-09-09T18:52:31.016559Z",
     "iopub.status.idle": "2025-09-09T18:52:31.034522Z",
     "shell.execute_reply": "2025-09-09T18:52:31.033558Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.017165Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pdf_articles_df = pd.DataFrame(train_pdf_articles, columns=[\"article_id\"])\n",
    "train_xml_articles_df = pd.DataFrame(train_xml_articles, columns=[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.035889Z",
     "iopub.status.busy": "2025-09-09T18:52:31.035586Z",
     "iopub.status.idle": "2025-09-09T18:52:31.057722Z",
     "shell.execute_reply": "2025-09-09T18:52:31.056748Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.035869Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pdf_articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.060437Z",
     "iopub.status.busy": "2025-09-09T18:52:31.058982Z",
     "iopub.status.idle": "2025-09-09T18:52:31.077393Z",
     "shell.execute_reply": "2025-09-09T18:52:31.076499Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.0604Z"
    }
   },
   "outputs": [],
   "source": [
    "train_xml_articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.078793Z",
     "iopub.status.busy": "2025-09-09T18:52:31.078416Z",
     "iopub.status.idle": "2025-09-09T18:52:31.102467Z",
     "shell.execute_reply": "2025-09-09T18:52:31.101576Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.078745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values for the dataset_id variable\n",
    "len(train_labels_df[train_labels_df[\"dataset_id\"]==\"Missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.103809Z",
     "iopub.status.busy": "2025-09-09T18:52:31.103473Z",
     "iopub.status.idle": "2025-09-09T18:52:31.124377Z",
     "shell.execute_reply": "2025-09-09T18:52:31.123371Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.103762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values for the type variable\n",
    "len(train_labels_df[train_labels_df[\"type\"]==\"Missing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Handle missing values\n",
    "2. Create an identification format(DOI or ACCESSION ID) for any mention\n",
    "3. Extract the raw text from the articles\n",
    "4. Extract all the training mentions from the article in their raw format:\n",
    "   \n",
    "   4.1. Regex based extractions\n",
    "   \n",
    "   4.2. Manuel corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.12618Z",
     "iopub.status.busy": "2025-09-09T18:52:31.125577Z",
     "iopub.status.idle": "2025-09-09T18:52:31.143734Z",
     "shell.execute_reply": "2025-09-09T18:52:31.14285Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.126148Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.145055Z",
     "iopub.status.busy": "2025-09-09T18:52:31.144714Z",
     "iopub.status.idle": "2025-09-09T18:52:31.164381Z",
     "shell.execute_reply": "2025-09-09T18:52:31.163549Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.145028Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_train_labels_df = train_labels_df[train_labels_df[\"type\"] != \"Missing\"]\n",
    "cleaned_train_labels_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.165826Z",
     "iopub.status.busy": "2025-09-09T18:52:31.165472Z",
     "iopub.status.idle": "2025-09-09T18:52:31.187791Z",
     "shell.execute_reply": "2025-09-09T18:52:31.186843Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.165802Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_train_labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an identification format(DOI or ACCESSION ID) for any mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.193749Z",
     "iopub.status.busy": "2025-09-09T18:52:31.193454Z",
     "iopub.status.idle": "2025-09-09T18:52:31.221402Z",
     "shell.execute_reply": "2025-09-09T18:52:31.220352Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.193726Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_doi_type(mention):\n",
    "    \n",
    "    doi_pattern = r'(https?://(?:dx\\.)?doi\\.org/10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+|doi:10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+)'\n",
    "    \n",
    "    return \"DOI\" if re.fullmatch(doi_pattern,mention,flags=re.IGNORECASE) else \"ACC\"\n",
    "\n",
    "cleaned_train_labels_df[\"dataset_id_type\"] = cleaned_train_labels_df.dataset_id.apply(add_doi_type)\n",
    "\n",
    "cleaned_train_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.222844Z",
     "iopub.status.busy": "2025-09-09T18:52:31.222456Z",
     "iopub.status.idle": "2025-09-09T18:52:31.257432Z",
     "shell.execute_reply": "2025-09-09T18:52:31.256348Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.222813Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df = cleaned_train_labels_df.query(\"dataset_id_type == 'DOI'\")\n",
    "dois_labels_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.259522Z",
     "iopub.status.busy": "2025-09-09T18:52:31.258628Z",
     "iopub.status.idle": "2025-09-09T18:52:31.284782Z",
     "shell.execute_reply": "2025-09-09T18:52:31.283738Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.259488Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_labels_df = cleaned_train_labels_df.query(\"dataset_id_type == 'ACC'\")\n",
    "acc_labels_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.286862Z",
     "iopub.status.busy": "2025-09-09T18:52:31.285927Z",
     "iopub.status.idle": "2025-09-09T18:52:31.304191Z",
     "shell.execute_reply": "2025-09-09T18:52:31.303271Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.286829Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_labels_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract the raw text from the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.305764Z",
     "iopub.status.busy": "2025-09-09T18:52:31.305432Z",
     "iopub.status.idle": "2025-09-09T18:52:31.328263Z",
     "shell.execute_reply": "2025-09-09T18:52:31.327322Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.305736Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExtractTextFromArticles:\n",
    "    \"\"\"\n",
    "    A utility class to extract raw text from a collection of scientific articles \n",
    "    stored in either XML or PDF formats. Supports batch extraction and export to JSON.\n",
    "\n",
    "    Attributes:\n",
    "        file_format (str): The format of the articles to process (\"xml\" or \"pdf\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_format: str):\n",
    "        \"\"\"\n",
    "        Initialize the extractor with a specified file format.\n",
    "        \n",
    "        Args:\n",
    "            file_format (str): The format of the articles (\"xml\" or \"pdf\").\n",
    "        \"\"\"\n",
    "        self.file_format = file_format.lower()\n",
    "\n",
    "        self.invisible_chars = re.compile(\n",
    "            r'[\\u00A0\\u200B\\u200C\\u200D\\u2060\\uFEFF\\u202F\\u00AD\\u2003\\x00-\\x09\\x0B-\\x0C\\x0E-\\x1F\\x7F\\xa0]')\n",
    "        self.invisible_spaces =  re.compile(r'[\\u00A0\\u2000-\\u200A\\u202F\\u205F]')     \n",
    "\n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "            \n",
    "    def remove_inv_chars(self, text: str) -> str:\n",
    "         \n",
    "        \"\"\"Remove invisible characters and normalize spaces.\"\"\"\n",
    "        if self.file_format ==\"pdf\":\n",
    "            text = self.invisible_spaces.sub(' ', text)\n",
    "            text = self.invisible_chars.sub('', text)\n",
    "        if self.file_format ==\"xml\":\n",
    "            text = re.sub(r'[^\\x20-\\x7E\\u000A\\u2013]', '', text)\n",
    "        return text   \n",
    "\n",
    "\n",
    "    def clean_articles(self, articles: List[Dict[str,str]]) -> Dict[str,str]:\n",
    "         \n",
    "        \"\"\"Remove invisible characters and normalize spaces in the text\"\"\"\n",
    "\n",
    "        cleaned = {d['file_name']:self.remove_inv_chars(d['raw_full_text']) for d in articles}\n",
    "\n",
    "        return cleaned  \n",
    "        \n",
    "   \n",
    "    def extract_all_texts(self, file_paths):\n",
    "        \"\"\"\n",
    "        Extract raw text from all files in the given list of file paths.\n",
    "\n",
    "        Args:\n",
    "            file_paths (List[Path]): List of Path objects pointing to XML or PDF files.\n",
    "\n",
    "        Returns:\n",
    "            map: A map object containing dictionaries with 'article_id' and 'raw_full_text'.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting extraction from {len(file_paths)} file(s)...\")\n",
    "        if self.file_format == \"xml\":\n",
    "            return map(self.from_xml_file, file_paths)\n",
    "        else:\n",
    "            return map(self.from_pdf_file, file_paths)\n",
    "        \n",
    "\n",
    "    def from_xml_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Extract all text content from a single XML file.\n",
    "\n",
    "        Args:\n",
    "            file_path (Path): Path to an XML file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with:\n",
    "                - 'article_id': file name without extension\n",
    "                - 'raw_full_text': all text content concatenated\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            text_content = ' '.join(root.itertext())\n",
    "            self.logger.debug(f\"Extracted XML: {file_path.name}\")\n",
    "            return {\n",
    "                \"file_name\": file_path.stem,\n",
    "                \"raw_full_text\": ' '.join(text_content.split())\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse XML: {file_path.name} — {e}\")\n",
    "            return {\"file_name\": file_path.stem, \"raw_full_text\": \"\"}\n",
    " \n",
    "\n",
    "    def from_pdf_file(self,file_path):\n",
    "        \"\"\"\n",
    "        Extract all text content from a single PDF file, removing repetitive headers and footers.\n",
    "    \n",
    "        Args:\n",
    "            file_path (Path): Path to a PDF file.\n",
    "    \n",
    "        Returns:\n",
    "            dict: {\n",
    "                - 'article_id': file name without extension\n",
    "                - 'raw_full_text': text without repeated headers/footers\n",
    "            }\n",
    "        \"\"\"\n",
    "        def is_informative(line: str) -> bool:\n",
    "            \n",
    "            line = line.strip()\n",
    "        \n",
    "            # Supprimer les lignes du type : \"5622  |\", \"14 |\", etc.\n",
    "            if re.fullmatch(r\"\\d+\\s*\\|\", line):\n",
    "                return False\n",
    "        \n",
    "            return True\n",
    "            \n",
    "        try:    \n",
    "            with fitz.open(file_path) as doc:\n",
    "                line_occurrences = collections.Counter()\n",
    "                page_lines_list = []\n",
    "    \n",
    "                # Step 1: collect all lines per page and count occurrences\n",
    "                for page in doc:\n",
    "                    lines = page.get_text().splitlines()\n",
    "                    page_lines_list.append(lines)\n",
    "                    line_occurrences.update(set(lines))  # use set to count once per page\n",
    "    \n",
    "                # Step 2: identify potential headers/footers (appear on many pages)\n",
    "                min_repeats = max(2, int(len(doc) * 0.7))  # heuristic: appears in >=70% of pages\n",
    "                common_lines = {line for line, count in line_occurrences.items() if count >= min_repeats}\n",
    "    \n",
    "                # Step 3: clean pages\n",
    "                cleaned_pages = []\n",
    "                for lines in page_lines_list:\n",
    "                    cleaned = [line for line in lines if line.strip() and line not in common_lines\n",
    "                              and is_informative(line)]\n",
    "                    cleaned_pages.append(\"\\n\".join(cleaned))\n",
    "    \n",
    "                full_text = \"\\f\".join(cleaned_pages)  # \\f = page separator (chr(12))\n",
    "    \n",
    "            self.logger.debug(f\"Extracted and cleaned PDF: {file_path.name}\")\n",
    "            return {\n",
    "                \"file_name\": file_path.stem,\n",
    "                \"raw_full_text\": full_text.strip()\n",
    "            }\n",
    "    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to extract PDF: {file_path.name} — {e}\")\n",
    "            return {\"file_name\": file_path.stem, \"raw_full_text\": \"\"}\n",
    "\n",
    "    def get_one_path(self, file_name: str, root_path: Path):\n",
    "        \"\"\"\n",
    "        Construct a full file path from a root path and a file name.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): The name of the file (e.g., \"article1.xml\").\n",
    "            root_path (Path): The directory containing the file.\n",
    "\n",
    "        Returns:\n",
    "            Path: Full path to the file.\n",
    "        \"\"\"\n",
    "        return root_path / file_name\n",
    "\n",
    "    def get_all_paths(self, root_path: Path, cleaned_train_article_ids: list):\n",
    "        \"\"\"\n",
    "        Retrieve all relevant file paths from a directory, optionally filtering by article ID.\n",
    "\n",
    "        Args:\n",
    "            root_path (Path): Directory containing the files.\n",
    "            cleaned_train_article_ids (list): List of article IDs (without extensions) to include.\n",
    "\n",
    "        Returns:\n",
    "            List[Path]: List of matching file paths.\n",
    "        \"\"\"\n",
    "        ext = \"pdf\" if self.file_format == \"pdf\" else \"xml\"\n",
    "        all_files = list(root_path.glob(f\"*.{ext}\"))\n",
    "\n",
    "        if cleaned_train_article_ids:\n",
    "            filtered = [self.get_one_path(art.name, root_path)\n",
    "                        for art in all_files\n",
    "                        if art.stem in cleaned_train_article_ids]\n",
    "            self.logger.info(f\"Filtered {len(filtered)} {ext.upper()} files out of {len(all_files)} based on article IDs.\")\n",
    "            return filtered\n",
    "        else:\n",
    "            self.logger.info(f\"Retrieved {len(all_files)} {ext.upper()} file(s).\")\n",
    "            return [self.get_one_path(art.name, root_path) for art in all_files]\n",
    "\n",
    "    def extract_text_data(self, file_paths):\n",
    "        \"\"\"\n",
    "        Extract all text content from the given files and return as a list of dicts,\n",
    "        without exporting to any file.\n",
    "    \n",
    "        Args:\n",
    "            file_paths (List[Path]): List of Path objects to XML or PDF files.\n",
    "    \n",
    "        Returns:\n",
    "            List[Dict]: List of dicts with 'article_id' and 'raw_full_text' keys.\n",
    "        \"\"\"\n",
    "        if not file_paths:\n",
    "            self.logger.warning(\"No files provided for extraction. Returning empty list.\")\n",
    "            return []\n",
    "    \n",
    "        self.logger.info(f\"Extracting texts from {len(file_paths)} file(s)...\")\n",
    "        all_texts = list(self.extract_all_texts(file_paths))\n",
    "    \n",
    "        self.logger.info(f\"Extraction complete. {len(all_texts)} article(s) extracted.\")\n",
    "        return all_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.3297Z",
     "iopub.status.busy": "2025-09-09T18:52:31.329381Z",
     "iopub.status.idle": "2025-09-09T18:52:31.354189Z",
     "shell.execute_reply": "2025-09-09T18:52:31.35323Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.329672Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_train_article_ids = cleaned_train_labels_df.article_id.drop_duplicates().tolist()\n",
    "\n",
    "cleaned_train_article_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.355544Z",
     "iopub.status.busy": "2025-09-09T18:52:31.355267Z",
     "iopub.status.idle": "2025-09-09T18:52:31.375186Z",
     "shell.execute_reply": "2025-09-09T18:52:31.374109Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.355523Z"
    }
   },
   "outputs": [],
   "source": [
    "xml_extractor = ExtractTextFromArticles(file_format=\"XML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.376559Z",
     "iopub.status.busy": "2025-09-09T18:52:31.376243Z",
     "iopub.status.idle": "2025-09-09T18:52:31.401639Z",
     "shell.execute_reply": "2025-09-09T18:52:31.400708Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.376528Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN \n",
    "cleaned_train_xml_paths = [xml_extractor.get_one_path(art.name, TRAIN_XML_FILES)  for art in TRAIN_XML_FILES.glob(\"*.xml\") if art.stem in cleaned_train_article_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.402928Z",
     "iopub.status.busy": "2025-09-09T18:52:31.402593Z",
     "iopub.status.idle": "2025-09-09T18:52:31.417527Z",
     "shell.execute_reply": "2025-09-09T18:52:31.416589Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.402903Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "test_xml_paths = [xml_extractor.get_one_path(art.name, TEST_XML_FILES)  for art in TEST_XML_FILES.glob(\"*.xml\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.418911Z",
     "iopub.status.busy": "2025-09-09T18:52:31.418524Z",
     "iopub.status.idle": "2025-09-09T18:52:31.42547Z",
     "shell.execute_reply": "2025-09-09T18:52:31.424655Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.418823Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_extractor = ExtractTextFromArticles(file_format=\"PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.426765Z",
     "iopub.status.busy": "2025-09-09T18:52:31.426448Z",
     "iopub.status.idle": "2025-09-09T18:52:31.453353Z",
     "shell.execute_reply": "2025-09-09T18:52:31.452358Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.426739Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN \n",
    "cleaned_train_pdf_paths = [pdf_extractor.get_one_path(art.name, TRAIN_PDF_FILES)  for art in TRAIN_PDF_FILES.glob(\"*.pdf\") if art.stem in cleaned_train_article_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.454798Z",
     "iopub.status.busy": "2025-09-09T18:52:31.454461Z",
     "iopub.status.idle": "2025-09-09T18:52:31.472791Z",
     "shell.execute_reply": "2025-09-09T18:52:31.471835Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.45475Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "test_pdf_paths = [pdf_extractor.get_one_path(art.name, TEST_PDF_FILES)  for art in TEST_PDF_FILES.glob(\"*.pdf\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### PDF ONLY ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.474151Z",
     "iopub.status.busy": "2025-09-09T18:52:31.473837Z",
     "iopub.status.idle": "2025-09-09T18:52:31.479225Z",
     "shell.execute_reply": "2025-09-09T18:52:31.478268Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.474124Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pdf_only(pdf_paths, xml_paths):\n",
    "\n",
    "    pdf_names = [fname.stem for fname in pdf_paths]\n",
    "\n",
    "    xml_names = [fname.stem for fname in xml_paths]\n",
    "\n",
    "    pdf_only_names = list(set(pdf_names).difference(set(xml_names)))\n",
    "\n",
    "    pdf_only_paths = [fpath for fpath in pdf_paths if fpath.stem in pdf_only_names]\n",
    "\n",
    "    return pdf_only_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.480642Z",
     "iopub.status.busy": "2025-09-09T18:52:31.480206Z",
     "iopub.status.idle": "2025-09-09T18:52:31.499987Z",
     "shell.execute_reply": "2025-09-09T18:52:31.499044Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.480615Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_train_pdf_only = get_pdf_only(cleaned_train_pdf_paths, cleaned_train_xml_paths)\n",
    "\n",
    "len(cleaned_train_pdf_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.501204Z",
     "iopub.status.busy": "2025-09-09T18:52:31.500901Z",
     "iopub.status.idle": "2025-09-09T18:52:31.520894Z",
     "shell.execute_reply": "2025-09-09T18:52:31.520047Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.501177Z"
    }
   },
   "outputs": [],
   "source": [
    "len(cleaned_train_pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.522343Z",
     "iopub.status.busy": "2025-09-09T18:52:31.521907Z",
     "iopub.status.idle": "2025-09-09T18:52:31.542979Z",
     "shell.execute_reply": "2025-09-09T18:52:31.542097Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.522315Z"
    }
   },
   "outputs": [],
   "source": [
    "len(cleaned_train_xml_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.544385Z",
     "iopub.status.busy": "2025-09-09T18:52:31.544084Z",
     "iopub.status.idle": "2025-09-09T18:52:31.563377Z",
     "shell.execute_reply": "2025-09-09T18:52:31.562566Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.544359Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pdf_only = get_pdf_only(test_pdf_paths, test_xml_paths)\n",
    "\n",
    "len(test_pdf_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrive all the training mentions for each article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Automated extraction : regex-based extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.564921Z",
     "iopub.status.busy": "2025-09-09T18:52:31.564569Z",
     "iopub.status.idle": "2025-09-09T18:52:31.5981Z",
     "shell.execute_reply": "2025-09-09T18:52:31.597096Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.564892Z"
    }
   },
   "outputs": [],
   "source": [
    "class DOIsFormatHandler:\n",
    "    \"\"\"\n",
    "    A handler class to extract, clean, normalize, and deduplicate DOIs (Digital Object Identifiers)\n",
    "    from raw text (e.g., full-text scientific articles).\n",
    "\n",
    "    This class is designed to process DOIs from text files with various formatting issues, \n",
    "    including invisible characters, inconsistent DOI formats, line breaks, and punctuation artifacts.\n",
    "\n",
    "    Attributes:\n",
    "        doi_pattern (str): Regex pattern used to match DOIs in text.\n",
    "        source_file_format (str): Format of the input source file, e.g., \"xml\".\n",
    "        invisible_chars (Pattern): Regex to detect invisible control characters.\n",
    "        invisible_spaces (Pattern): Regex to detect various non-breaking/invisible spaces.\n",
    "        modern_doi_format_https (str): Regex for HTTPS modern DOI format.\n",
    "        modern_doi_format_http (str): Regex for HTTP modern DOI format.\n",
    "        old_doi_format_https (str): Regex for old-style HTTPS DOI format.\n",
    "        old_doi_format_http (str): Regex for old-style HTTP DOI format.\n",
    "        short_doi_format (str): Regex for abbreviated DOI formats.\n",
    "        very_short_doi_format (str): Regex for minimal DOIs with just prefix/suffix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doi_pattern=None, source_file_format=\"xml\", invisible_chars=None, invisible_spaces=None):\n",
    "        # Use default regex pattern for DOIs if none is provided\n",
    "        self.doi_pattern = doi_pattern or r'''(?ix)\n",
    "            (?:https?\\s*:\\s*/\\s*/\\s*(?:dx\\s*\\.\\s*)?doi\\s*\\.\\s*org\\s*/\\s*|\n",
    "               (?:dx\\s*\\.\\s*)?doi\\s*\\.\\s*org\\s*/\\s*|\n",
    "               doi\\s*:\\s*)?\n",
    "            10\\s*\\.\\s*\\d{4,9}\n",
    "            (?:\\s*/\\s*[\\w\\-]+(?:\\s*[\\./\\-]\\s*[\\w\\-]+)*)'''\n",
    "\n",
    "        # Patterns to match and remove invisible characters and spaces\n",
    "        self.invisible_chars = invisible_chars or re.compile(\n",
    "            r'[\\u00A0\\u200B\\u200C\\u200D\\u2060\\uFEFF\\u202F\\u00AD\\u2003\\x00-\\x09\\x0B-\\x0C\\x0E-\\x1F\\x7F\\xa0]')\n",
    "        self.invisible_spaces = invisible_spaces or re.compile(r'[\\u00A0\\u2000-\\u200A\\u202F\\u205F]')\n",
    "\n",
    "        # DOI format regexes for validation and normalization\n",
    "        self.modern_doi_format_https = r'https://doi\\.org/10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+'\n",
    "        self.modern_doi_format_http = r'http://doi\\.org/10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+'\n",
    "        self.old_doi_format_https = r'https://dx\\.doi\\.org/10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+'\n",
    "        self.old_doi_format_http = r'http://dx\\.doi\\.org/10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+'\n",
    "\n",
    "        # Short and very short DOI formats\n",
    "        self.short_doi_format = r'''(?ix)\n",
    "            (\n",
    "                (?:doi:|dx\\.doi\\.org/|doi\\.org/)\n",
    "                10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+\n",
    "            )\n",
    "            \\.?'''\n",
    "        self.very_short_doi_format = r'10\\.\\d{4,9}/[^\\s\"<>]+'\n",
    "\n",
    "        self.source_file_format = source_file_format\n",
    "\n",
    "    def extract_dois(self, text: str):\n",
    "        \"\"\"\n",
    "        Extracts and cleans all DOIs from the given text.\n",
    "        \"\"\"\n",
    "        extracted_dois = []\n",
    "\n",
    "        # Clean the text of invisible or control characters\n",
    "        text = self.remove_inv_chars(text)\n",
    "\n",
    "        # Find all potential DOIs using regex\n",
    "        matches = list(re.finditer(self.doi_pattern, text, re.IGNORECASE | re.VERBOSE))\n",
    "\n",
    "        extracted = []\n",
    "        \n",
    "        for m in matches:\n",
    "            raw_ext = m.group()\n",
    "            start = m.start()\n",
    "            end = m.end()\n",
    "            \n",
    "            raw_doi = self.clean_extraction(raw_ext)\n",
    " \n",
    "            # raw_doi = raw_ext\n",
    "            \n",
    "            cleaned_doi = self.remove_trailing_spaces(\n",
    "                self.remove_ligatures(\n",
    "                    self.remove_line_break(raw_doi)\n",
    "                )\n",
    "            )\n",
    "            extracted_dois.append({\n",
    "                \"raw_extraction\": raw_ext,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"raw_doi\": raw_doi,\n",
    "                \"cleaned_doi\": cleaned_doi\n",
    "            })\n",
    "\n",
    "        return extracted_dois\n",
    "\n",
    "    def contains_no_digits(self, sequence):\n",
    "        \"\"\"Check if a given sequence contains no digits.\"\"\"\n",
    "        return not re.search(r'\\d', sequence)\n",
    "\n",
    "    def remove_chars_after_period(self, raw_extraction):\n",
    "        \"\"\"\n",
    "        Recursively removes the sentence-like fragment after a period if it contains no digits.\n",
    "        Helps isolate the DOI from trailing sentence fragments.\n",
    "        \"\"\"\n",
    "        segments = raw_extraction.split(\". \")\n",
    "        if self.contains_no_digits(segments[-1]):\n",
    "            if len(segments) >= 2:\n",
    "                to_remove = \". \" + segments[-1]\n",
    "                if raw_extraction.endswith(to_remove):\n",
    "                    raw_extraction = raw_extraction[: -len(to_remove)].rstrip()\n",
    "                    if len(segments) > 2:\n",
    "                        return self.remove_chars_after_period(raw_extraction)\n",
    "        return raw_extraction\n",
    "\n",
    "    def _contains_reference_num(self, sequence):\n",
    "        \"\"\"\n",
    "        Detects reference-like numbering (e.g., '1. Smith') at line endings.\n",
    "        \"\"\"\n",
    "        segments = sequence.replace(\" \", \"\").split('.')\n",
    "        return len(segments) == 2 and segments[0].isdigit() and self.contains_no_digits(segments[1])\n",
    "\n",
    "    def remove_reference_num(self, raw_extraction):\n",
    "        \"\"\"Removes reference number line endings if detected.\"\"\"\n",
    "        segments = raw_extraction.split(\"\\n\")\n",
    "        last_element = segments[-1]\n",
    "        if self._contains_reference_num(last_element):\n",
    "            return raw_extraction[: -len(\"\\n\" + last_element)].rstrip()\n",
    "        return raw_extraction\n",
    "\n",
    "    def remove_chars_after_line_break(self, raw_extraction):\n",
    "        \"\"\"Removes line-break suffixes that are not DOI content.\"\"\"\n",
    "        while True:\n",
    "            segments = raw_extraction.split(\"\\n\")\n",
    "            last_element = segments[-1]\n",
    "            if self.contains_no_digits(last_element) or last_element.upper().startswith(\"GBIF\"):\n",
    "                if raw_extraction.endswith(\"\\n\" + last_element):\n",
    "                    raw_extraction = raw_extraction[: -len(\"\\n\" + last_element)].rstrip()\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        return raw_extraction\n",
    "\n",
    "    def clean_very_short_format(self, raw_extraction):\n",
    "        \"\"\"\n",
    "        Handles cleaning of very short DOI patterns that might have extra trailing text.\n",
    "        \"\"\"\n",
    "        raw_doi = self.remove_line_break(raw_extraction).replace(\" \", \"\")\n",
    "        if  re.fullmatch(self.very_short_doi_format, raw_doi):\n",
    "            segments = re.split(self.very_short_doi_format, raw_extraction)\n",
    "            if len(segments) == 2 and segments[0] == \"\":\n",
    "                if segments[-1]:\n",
    "                    raw_extraction = raw_extraction.rstrip(segments[-1])\n",
    "        return raw_extraction\n",
    "\n",
    "    def clean_extraction(self, raw_extraction):\n",
    "        \"\"\"\n",
    "        Performs a full cleaning pipeline on a raw DOI extraction.\n",
    "        \"\"\"\n",
    "        raw_extraction = self.remove_reference_num(raw_extraction)\n",
    "        raw_extraction = self.remove_chars_after_line_break(raw_extraction)\n",
    "        raw_extraction = self.remove_chars_after_period(raw_extraction)\n",
    "        raw_extraction = self.clean_very_short_format(raw_extraction)\n",
    "        return self._remove_last_period(raw_extraction)\n",
    "\n",
    "    def _remove_last_period(self, raw_doi):\n",
    "        \"\"\"Remove trailing period (\".\") if it exists.\"\"\"\n",
    "        return raw_doi[:-1].rstrip() if raw_doi and raw_doi.endswith(\".\") else raw_doi\n",
    "\n",
    "    def normalize_doi(self, dois_dict):\n",
    "        \"\"\"\n",
    "        Normalizes DOI to a standard HTTPS format (e.g., https://doi.org/...)\n",
    "        \"\"\"\n",
    "        cleaned_doi = dois_dict[\"cleaned_doi\"]\n",
    "\n",
    "        if not cleaned_doi:\n",
    "            return \"\"\n",
    "        doi = cleaned_doi.strip().replace('\\n', '').replace(' ', '')\n",
    "        doi = re.sub(r'^(doi:|DOI:)', '', doi, flags=re.IGNORECASE)\n",
    "        doi = re.sub(r'^(https?://)?(dx\\./?)?doi\\.org/', '', doi, flags=re.IGNORECASE)\n",
    "        \n",
    "        normalized_doi= f\"https://doi.org/{doi}\"\n",
    "\n",
    "        dois_dict.update({\"normalized_doi\": normalized_doi})\n",
    "        return dois_dict\n",
    "\n",
    "\n",
    "    def remove_trailing_spaces(self, raw_doi):\n",
    "        \"\"\"Remove all spaces from the DOI string.\"\"\"\n",
    "        return re.sub(\" \", \"\", raw_doi)\n",
    "\n",
    "    def remove_inv_chars(self, text):\n",
    "        \"\"\"Remove invisible characters and normalize spaces.\"\"\"\n",
    "        text = self.invisible_spaces.sub(' ', text)\n",
    "        text = self.invisible_chars.sub('', text)\n",
    "        return text\n",
    "\n",
    "    def remove_line_break(self, raw_doi):\n",
    "        \"\"\"Remove all line breaks from a DOI string.\"\"\"\n",
    "        return re.sub(r\"\\n\", \" \", raw_doi)\n",
    "\n",
    "    def remove_ligatures(self, raw_doi):\n",
    "        \"\"\"Normalize ligatures and combined characters.\"\"\"\n",
    "        return unicodedata.normalize('NFKD', raw_doi)\n",
    "\n",
    "    def add_doi_type(self, mention):\n",
    "        \"\"\"\n",
    "        Classifies string as DOI or ACCESSION based on its format.\n",
    "        \"\"\"\n",
    "        return \"DOI\" if re.fullmatch(self.modern_doi_format_https, mention, flags=re.IGNORECASE) else \"ACCESSION\"\n",
    "\n",
    "    def _replace_invisible_spaces(self, text):\n",
    "        \"\"\"\n",
    "        Replace various invisible space-like characters with a normal space.\n",
    "        \"\"\"\n",
    "        invisible_spaces = ['\\u00A0', '\\u202F', '\\u2007', '\\u2060', '\\u200B']\n",
    "        for char in invisible_spaces:\n",
    "            text = text.replace(char, ' ')\n",
    "        return text\n",
    "\n",
    "    def remove_duplicates(self, dois_dicts):\n",
    "        \"\"\"\n",
    "        Deduplicates DOI entries based on their normalized DOI string.\n",
    "        \"\"\"\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for d in dois_dicts:\n",
    "            if d[\"normalized_doi\"] not in seen:\n",
    "                seen.add(d[\"normalized_doi\"])\n",
    "                result.append(d)\n",
    "        return result\n",
    "\n",
    "    def process_article(self, art_dict):\n",
    "        \"\"\"\n",
    "        Main method to process a single article's text and extract all cleaned and normalized DOIs.\n",
    "\n",
    "        Args:\n",
    "            art_dict (dict): A dictionary with at least 'article_id' and 'raw_full_text'.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains the article ID and list of extracted DOI dictionaries.\n",
    "        \"\"\"\n",
    "        file_name = art_dict.get(\"file_name\")\n",
    "        text = art_dict.get(\"raw_full_text\")\n",
    "        raw_dois = self.extract_dois(text)\n",
    "        normalized_dois = list(map(self.normalize_doi, raw_dois))\n",
    "        cleaned_dois = self.remove_duplicates(normalized_dois)\n",
    "        return {\"file_name\": file_name, \"extracted_dois\": cleaned_dois}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. PDF FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.59952Z",
     "iopub.status.busy": "2025-09-09T18:52:31.599209Z",
     "iopub.status.idle": "2025-09-09T18:52:31.622813Z",
     "shell.execute_reply": "2025-09-09T18:52:31.621855Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.599495Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_dois_handler = DOIsFormatHandler(source_file_format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:52:31.624295Z",
     "iopub.status.busy": "2025-09-09T18:52:31.623821Z",
     "iopub.status.idle": "2025-09-09T18:53:16.289918Z",
     "shell.execute_reply": "2025-09-09T18:53:16.289112Z",
     "shell.execute_reply.started": "2025-09-09T18:52:31.624272Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_train_texts = pdf_extractor.extract_text_data(cleaned_train_pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:16.291118Z",
     "iopub.status.busy": "2025-09-09T18:53:16.290762Z",
     "iopub.status.idle": "2025-09-09T18:53:18.575168Z",
     "shell.execute_reply": "2025-09-09T18:53:18.574296Z",
     "shell.execute_reply.started": "2025-09-09T18:53:16.291092Z"
    }
   },
   "outputs": [],
   "source": [
    "all_dois_from_pdfs=[pdf_dois_handler.process_article(art) for art in pdf_train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:18.576319Z",
     "iopub.status.busy": "2025-09-09T18:53:18.576023Z",
     "iopub.status.idle": "2025-09-09T18:53:18.882741Z",
     "shell.execute_reply": "2025-09-09T18:53:18.881899Z",
     "shell.execute_reply.started": "2025-09-09T18:53:18.576294Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_clean_train_texts = pdf_extractor.clean_articles(pdf_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:18.884128Z",
     "iopub.status.busy": "2025-09-09T18:53:18.883824Z",
     "iopub.status.idle": "2025-09-09T18:53:18.890633Z",
     "shell.execute_reply": "2025-09-09T18:53:18.889843Z",
     "shell.execute_reply.started": "2025-09-09T18:53:18.884086Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_normalized_dois(all_dois, cleaned_train_article_ids):\n",
    "    \"\"\"\n",
    "    Extracts and organizes normalized DOIs from a list of article DOI data.\n",
    "\n",
    "    For each article in the input list, this function:\n",
    "    - Collects the normalized DOIs (in lowercase) if the article ID is in `cleaned_train_article_ids`.\n",
    "    - Creates a mapping from each normalized DOI to its corresponding raw DOI string.\n",
    "\n",
    "    Args:\n",
    "        all_dois (list): A list of dictionaries, each containing:\n",
    "            - \"article_id\" (str): The unique identifier for the article.\n",
    "            - \"extracted_dois\" (list): A list of DOI dictionaries with at least:\n",
    "                - \"normalized_doi\" (str): The normalized DOI.\n",
    "                - \"raw_doi\" (str): The original, unprocessed DOI string.\n",
    "        \n",
    "        cleaned_train_article_ids (set or list): A collection of article IDs to include in the output.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - dict: Keys are article IDs, values are lists of lowercase normalized DOIs\n",
    "                    (only for articles in `cleaned_train_article_ids`).\n",
    "            - dict: Keys are article IDs, values are dictionaries mapping normalized DOIs (lowercase)\n",
    "                    to their corresponding raw DOIs.\n",
    "    \"\"\"\n",
    "    normalized = {\n",
    "        article[\"file_name\"]: [\n",
    "            doi[\"normalized_doi\"].lower()\n",
    "            for doi in article[\"extracted_dois\"]\n",
    "        ]\n",
    "        for article in all_dois\n",
    "        if article[\"file_name\"] in cleaned_train_article_ids\n",
    "    }\n",
    "\n",
    "    norm_raw_mapping = {\n",
    "        article[\"file_name\"]: {\n",
    "            doi[\"normalized_doi\"].lower(): doi[\"raw_doi\"]\n",
    "            for doi in article[\"extracted_dois\"]\n",
    "        }\n",
    "        for article in all_dois\n",
    "    }\n",
    "\n",
    "    return normalized, norm_raw_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:18.891922Z",
     "iopub.status.busy": "2025-09-09T18:53:18.891612Z",
     "iopub.status.idle": "2025-09-09T18:53:18.92071Z",
     "shell.execute_reply": "2025-09-09T18:53:18.919838Z",
     "shell.execute_reply.started": "2025-09-09T18:53:18.891897Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_normalized_dois, norm_raw_mapping  = get_normalized_dois(all_dois_from_pdfs,cleaned_train_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:18.922121Z",
     "iopub.status.busy": "2025-09-09T18:53:18.921761Z",
     "iopub.status.idle": "2025-09-09T18:53:18.947684Z",
     "shell.execute_reply": "2025-09-09T18:53:18.946742Z",
     "shell.execute_reply.started": "2025-09-09T18:53:18.922094Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_identification(article_id, dataset_id, normalized_dois, norm_raw_map):\n",
    "    \"\"\"\n",
    "    Checks whether a given dataset ID matches a normalized DOI from an article.\n",
    "\n",
    "    The function verifies if the `dataset_id`:\n",
    "    1. Exactly matches one of the normalized DOIs for the article.\n",
    "    2. Closely matches (using fuzzy string matching) a DOI if there's no exact match.\n",
    "\n",
    "    Args:\n",
    "        article_id (str): The unique identifier of the article to check.\n",
    "        dataset_id (str): The DOI or identifier to validate against the article's DOIs.\n",
    "        normalized_dois (dict): Mapping of article_id → list of normalized DOIs (lowercase).\n",
    "        norm_raw_map (dict): Mapping of article_id → {normalized DOI → raw DOI}.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - bool: True if an exact match is found, False otherwise.\n",
    "            - str or None: The matched DOI (exact or closest match), or None if no match.\n",
    "            - str or None: The raw DOI corresponding to the matched normalized DOI, or None.\n",
    "    \"\"\"\n",
    "    art_norm_dois = normalized_dois.get(article_id, [])\n",
    "\n",
    "    if not art_norm_dois:\n",
    "        return False, None, None\n",
    "\n",
    "    if dataset_id in art_norm_dois:\n",
    "        # Exact match found\n",
    "        raw_doi = norm_raw_map.get(article_id, {}).get(dataset_id)\n",
    "        return True, dataset_id, raw_doi\n",
    "\n",
    "    # Attempt fuzzy match if no exact match\n",
    "    close_matches = difflib.get_close_matches(dataset_id, art_norm_dois, n=1, cutoff=0.8)\n",
    "    match = close_matches[0] if close_matches else None\n",
    "\n",
    "    raw_match = norm_raw_map.get(article_id, {}).get(match) if match else None\n",
    "\n",
    "    return False, match, raw_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:18.948914Z",
     "iopub.status.busy": "2025-09-09T18:53:18.948608Z",
     "iopub.status.idle": "2025-09-09T18:53:19.013629Z",
     "shell.execute_reply": "2025-09-09T18:53:19.012762Z",
     "shell.execute_reply.started": "2025-09-09T18:53:18.94888Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df2 = dois_labels_df.copy()\n",
    "\n",
    "dois_labels_df2[[\"is_well_identified\", \"normalized_match\", \"raw_match\"]] = dois_labels_df2.apply(\n",
    "    \n",
    "    lambda row: check_identification(row[\"article_id\"], row[\"dataset_id\"],clean_normalized_dois,norm_raw_mapping),\n",
    "    \n",
    "    axis=1,\n",
    "    \n",
    "    result_type='expand'  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:19.014751Z",
     "iopub.status.busy": "2025-09-09T18:53:19.01443Z",
     "iopub.status.idle": "2025-09-09T18:53:19.028188Z",
     "shell.execute_reply": "2025-09-09T18:53:19.027192Z",
     "shell.execute_reply.started": "2025-09-09T18:53:19.014721Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. XML FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:19.029706Z",
     "iopub.status.busy": "2025-09-09T18:53:19.028903Z",
     "iopub.status.idle": "2025-09-09T18:53:21.117621Z",
     "shell.execute_reply": "2025-09-09T18:53:21.116809Z",
     "shell.execute_reply.started": "2025-09-09T18:53:19.029679Z"
    }
   },
   "outputs": [],
   "source": [
    "xml_train_texts = xml_extractor.extract_text_data(cleaned_train_xml_paths)\n",
    "\n",
    "xml_clean_train_texts = xml_extractor.clean_articles(xml_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.118885Z",
     "iopub.status.busy": "2025-09-09T18:53:21.118535Z",
     "iopub.status.idle": "2025-09-09T18:53:21.127418Z",
     "shell.execute_reply": "2025-09-09T18:53:21.126809Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.118769Z"
    }
   },
   "outputs": [],
   "source": [
    "not_identified_dois = dois_labels_df2[dois_labels_df2.is_well_identified == False]\n",
    "\n",
    "not_identified_dois.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.129013Z",
     "iopub.status.busy": "2025-09-09T18:53:21.128711Z",
     "iopub.status.idle": "2025-09-09T18:53:21.155584Z",
     "shell.execute_reply": "2025-09-09T18:53:21.154851Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.12899Z"
    }
   },
   "outputs": [],
   "source": [
    "not_identified_dois.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following mentions are present in the corresponding XML files**:\n",
    "'10.7554_elife.63455'\t: 'https://doi.org/10.5061/dryad.37pvmcvj9',\n",
    "'10.7554_elife.63455' :\t'https://doi.org/10.5061/dryad.qnk98sffp',\n",
    "'10.7554_elife.73695':\t'https://doi.org/10.5441/001/1.3q2131q5',\n",
    "'10.7554_elife.74937'\t:'https://doi.org/10.5281/zenodo.6335347'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.156934Z",
     "iopub.status.busy": "2025-09-09T18:53:21.156582Z",
     "iopub.status.idle": "2025-09-09T18:53:21.193103Z",
     "shell.execute_reply": "2025-09-09T18:53:21.19217Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.156909Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_found_in_xml = {\n",
    "'10.7554_elife.63455&https://doi.org/10.5061/dryad.37pvmcvj9'\t:['https://doi.org/10.5061/dryad.37pvmcvj9',\n",
    "                                                                 '10.5061/dryad.37pvmcvj9',True],\n",
    "'10.7554_elife.63455&https://doi.org/10.5061/dryad.qnk98sffp' :['https://doi.org/10.5061/dryad.qnk98sffp',\n",
    "                                                                '10.5061/dryad.qnk98sffp',True],\n",
    "'10.7554_elife.73695&https://doi.org/10.5441/001/1.3q2131q5':['https://doi.org/10.5441/001/1.3q2131q5',\n",
    "                                                             'https://doi.org/10.5441/001/1.3q2131q5',True],\n",
    "'10.7554_elife.74937&https://doi.org/10.5281/zenodo.6335347':['https://doi.org/10.5281/zenodo.6335347',\n",
    "                                                                  'https://doi.org/10.5281/zenodo.6335347',True]\n",
    "}\n",
    "\n",
    "dois_found_in_xml_tuples =     [\n",
    "('10.7554_elife.63455','10.5061/dryad.37pvmcvj9'),\n",
    "('10.7554_elife.63455','10.5061/dryad.qnk98sffp'),\n",
    "('10.7554_elife.73695','https://doi.org/10.5441/001/1.3q2131q5'),\n",
    "('10.7554_elife.74937','https://doi.org/10.5281/zenodo.6335347')\n",
    "    ]\n",
    "\n",
    "for k, v in dois_found_in_xml.items():\n",
    "\n",
    "    article_id, dataset_id = k.split(\"&\")\n",
    "    \n",
    "    index = dois_labels_df2[(dois_labels_df2.article_id ==article_id) & (dois_labels_df2.dataset_id ==dataset_id)].index\n",
    "\n",
    "    dois_labels_df2.loc[index,['normalized_match',\"raw_match\", 'is_well_identified']]= v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Manual corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.194936Z",
     "iopub.status.busy": "2025-09-09T18:53:21.193996Z",
     "iopub.status.idle": "2025-09-09T18:53:21.216717Z",
     "shell.execute_reply": "2025-09-09T18:53:21.215901Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.194907Z"
    }
   },
   "outputs": [],
   "source": [
    "not_identified_dois = dois_labels_df2[dois_labels_df2.is_well_identified == False]\n",
    "\n",
    "not_identified_dois.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 42 non catched DOI mentions**. There are differents situations :  \n",
    "1. some mentions don't exist in the articles (pdf format) even though they are given in the training data\n",
    "2. some mentions are incorrect in the training data : v2 instead of v1 for example\n",
    "3. some correct mentions are not catched by the regex based fonction : because of line breaks for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.218185Z",
     "iopub.status.busy": "2025-09-09T18:53:21.217718Z",
     "iopub.status.idle": "2025-09-09T18:53:21.237392Z",
     "shell.execute_reply": "2025-09-09T18:53:21.236507Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.218158Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_identified_art = not_identified_dois[not_identified_dois.normalized_match.isnull()].article_id.values.tolist()\n",
    "not_identified_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 => We remove these articles from the training data because they are not found in the corresponding pdf articles. One of them (10.1590_1809-9823.2007.10034) is not in English\n",
    "\n",
    "Articles to remove :  \n",
    "['10.1016_j.cpc.2024.109087',\n",
    " '10.1021_acs.jcim.9b01185',\n",
    " '10.1038_s41558-022-01301-z',\n",
    " '10.1111_1365-2656.12594',\n",
    " '10.3390_s19030479']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.238613Z",
     "iopub.status.busy": "2025-09-09T18:53:21.238316Z",
     "iopub.status.idle": "2025-09-09T18:53:21.25369Z",
     "shell.execute_reply": "2025-09-09T18:53:21.252841Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.238584Z"
    }
   },
   "outputs": [],
   "source": [
    "art_to_remove = ['10.1016_j.cpc.2024.109087',\n",
    " '10.1021_acs.jcim.9b01185',\n",
    " '10.1038_s41558-022-01301-z',\n",
    " '10.3390_s19030479']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.254936Z",
     "iopub.status.busy": "2025-09-09T18:53:21.254652Z",
     "iopub.status.idle": "2025-09-09T18:53:21.275165Z",
     "shell.execute_reply": "2025-09-09T18:53:21.274302Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.254907Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.\n",
    "dois_labels_df2 = dois_labels_df2.loc[~dois_labels_df2.article_id.isin(art_to_remove),:]\n",
    "\n",
    "dois_labels_df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dois_labels_df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 => We'll keep the DOIs in the same format that they are mentionned in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'https://doi.org/10.17862/cranfield.rd.19146182' => 'https://doi.org/10.17862/cranfield.rd.19146182.v1'  \n",
    "\n",
    "'https://doi.org/10.11583/dtu.20555586' => 'https://doi.org/10.11583/dtu.20555586.v2'  \n",
    "\n",
    "'https://doi.org/10.11583/dtu.20555586.v3' => 'https://doi.org/10.11583/dtu.20555586.v2'  \n",
    "\n",
    "'https://doi.org/10.23642/usn.15134442'  => 'https://doi.org/10.23642/usn.15134442.v1'  \n",
    "\n",
    "'https://doi.org/10.5281/zenodo.8014149' => 'https://doi.org/10.5281/zenodo.8014150'  \n",
    "\n",
    "'https://doi.org/10.25377/sussex.21184705.v1' => 'https://doi.org/10.25377/sussex.21184705'  \n",
    "\n",
    "'https://doi.org/10.5281/zenodo.1472499' =>'https://doi.org/10.5281/zenodo.1472500'\t  \n",
    "\n",
    "'https://doi.org/10.5281/zenodo.1135371' => 'https://doi.org/10.5281/zenodo.1135372'\n",
    "\n",
    "'https://doi.org/10.5285/378f0f77-1842-4789-ba15-6fbdf7d02299' => 'http://\\ndx.doi.org/10.5061/dryad.gb5mkkwk8'\n",
    "\n",
    "'https://doi.org/10.5061/dryad.ns1rn8pnt' => 'http://doi.pangaea.de/10.1594/PANGAEA.806957' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 => We fix the extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.276483Z",
     "iopub.status.busy": "2025-09-09T18:53:21.27616Z",
     "iopub.status.idle": "2025-09-09T18:53:21.293523Z",
     "shell.execute_reply": "2025-09-09T18:53:21.292512Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.276464Z"
    }
   },
   "outputs": [],
   "source": [
    "to_change = {\n",
    "    '10.5256/f1000research. \\n13622.d19423315':'10.5256/f1000research. \\n13622.d194233',\n",
    "    'https://doi.org/10.5066/P9VYSWEH.\\n64': 'https://doi.org/10.5066/P9VYSWEH',\n",
    "    'https://doi.org/10.5066/P9H1S1PV.\\n34':'https://doi.org/10.5066/P9H1S1PV',\n",
    "    'https://doi.org/10.5066/P9FZ4OJW.\\n54':'https://doi.org/10.5066/P9FZ4OJW',\n",
    "    'https://doi.org/10.5066/P92BGHW1.\\n62':'https://doi.org/10.5066/P92BGHW1',\n",
    "    'https://doi.org/10.5061/dryad.g1jws': 'https://doi.org/10.5061/dryad.g1jws\\ntqsg',\n",
    "    'https://doi.org/10.17638/datacat.\\nliverpool.ac.uk/417.\\n37': 'https://doi.org/10.17638/datacat.\\nliverpool.ac.uk/417',\n",
    "    'https://doi.org/10.5281/zenodo.10908593/': 'https://doi.org/10.5281/zenodo.10908593',\n",
    "    'https://doi.org/10.5281/zenodo.817658.\\n856': 'https://doi.org/10.5281/zenodo.817658',\n",
    "    'DOI: 10.5281/zenodo.6010342.\\n29':'DOI: 10.5281/zenodo.6010342',\n",
    "    'DOI: 10.5281/zenodo.6010342.\\n29':'DOI: 10.5281/zenodo.6010342',\n",
    "    'https://doi.org/10.6073/pasta/be42bb841e696b7bca':'https://doi.org/10.6073/pasta/be42bb841e696b7bca\\nd9957aed33db5e',\n",
    "    'https://doi.org/10.5061/dryad':'https://doi.org/10.5061/dryad.\\nwpzgmsbps',\n",
    "    'https://doi.org/10.6073/pasta/\\nbb935444378d112d9189556fd22a441d17':'https://doi.org/10.6073/pasta/\\nbb935444378d112d9189556fd22a441d',\n",
    "    'https://\\ndoi.org/10.15468/dl.yu2xpvSara':'https://\\ndoi.org/10.15468/dl.yu2xpv',\n",
    "    'https://doi.\\norg/10.13012/B2IDB-5784165_V1.reports': 'https://doi.\\norg/10.13012/B2IDB-5784165_V1',\n",
    "    'https://\\ndoi .org /10 .17862 /cranﬁeld .rd .19146182 .v1':'https://doi .org /10 .17862 /cranﬁeld .rd .\\n19146182 .v1'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the '10.3389_fevo.2023.1112519' article, I found the following mentions :  \n",
    "- 'doi.org/10.1594/PANGAEA.921544' => DOI\n",
    "- 10.1594/PANGAEA.941237' => DOI\n",
    "- 'PRJNA780103'=> ACCESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.295049Z",
     "iopub.status.busy": "2025-09-09T18:53:21.294609Z",
     "iopub.status.idle": "2025-09-09T18:53:21.320161Z",
     "shell.execute_reply": "2025-09-09T18:53:21.319137Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.294973Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df2['raw_dataset_id'] = dois_labels_df2[\"raw_match\"]\n",
    "\n",
    "dois_labels_df2.replace({'raw_dataset_id' : to_change},inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.32134Z",
     "iopub.status.busy": "2025-09-09T18:53:21.321097Z",
     "iopub.status.idle": "2025-09-09T18:53:21.337871Z",
     "shell.execute_reply": "2025-09-09T18:53:21.336995Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.32132Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df2.at[67, 'raw_dataset_id'] = 'http://doi.pangaea.de/10.1594/PANGAEA.806957'\n",
    "\n",
    "dois_labels_df2.at[136, 'raw_dataset_id'] = 'http://\\ndx.doi.org/10.5061/dryad.gb5mkkwk8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.339129Z",
     "iopub.status.busy": "2025-09-09T18:53:21.338794Z",
     "iopub.status.idle": "2025-09-09T18:53:21.360107Z",
     "shell.execute_reply": "2025-09-09T18:53:21.359238Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.339101Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_normalized_doi(dataset_id_type, dataset_id):\n",
    "\n",
    "    if str(dataset_id_type) == 'DOI':\n",
    "\n",
    "        normalized = pdf_dois_handler.normalize_doi({'cleaned_doi':str(dataset_id).replace('\\n','').lower()})\n",
    "\n",
    "        return normalized['normalized_doi']\n",
    "\n",
    "    else:\n",
    "\n",
    "        return dataset_id\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.361299Z",
     "iopub.status.busy": "2025-09-09T18:53:21.361061Z",
     "iopub.status.idle": "2025-09-09T18:53:21.392597Z",
     "shell.execute_reply": "2025-09-09T18:53:21.391615Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.361281Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_to_change = dois_labels_df2[dois_labels_df2.article_id == '10.3389_fevo.2023.1112519'].index.tolist()\n",
    "\n",
    "indices_to_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.393908Z",
     "iopub.status.busy": "2025-09-09T18:53:21.393572Z",
     "iopub.status.idle": "2025-09-09T18:53:21.412657Z",
     "shell.execute_reply": "2025-09-09T18:53:21.411698Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.39388Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df2.loc[indices_to_change, 'raw_dataset_id'] = ['https://doi.\\npangaea.de/10.1594/PANGAEA.941237',\n",
    "                                                            'doi.org/10.1594/PANGAEA.921544',\n",
    "                                                            'PRJNA780103'\n",
    "                                                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.414186Z",
     "iopub.status.busy": "2025-09-09T18:53:21.413762Z",
     "iopub.status.idle": "2025-09-09T18:53:21.4326Z",
     "shell.execute_reply": "2025-09-09T18:53:21.431815Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.41416Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df2.loc[indices_to_change[-1], 'dataset_id_type'] = 'ACC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.434548Z",
     "iopub.status.busy": "2025-09-09T18:53:21.434266Z",
     "iopub.status.idle": "2025-09-09T18:53:21.461213Z",
     "shell.execute_reply": "2025-09-09T18:53:21.460239Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.434529Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df2['normalized_dataset_id'] = dois_labels_df2.apply(\n",
    "    \n",
    "    lambda row: get_normalized_doi(row[\"dataset_id_type\"], row[\"raw_dataset_id\"]),\n",
    "    \n",
    "    axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.462428Z",
     "iopub.status.busy": "2025-09-09T18:53:21.462126Z",
     "iopub.status.idle": "2025-09-09T18:53:21.482632Z",
     "shell.execute_reply": "2025-09-09T18:53:21.481653Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.462398Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df3 = dois_labels_df2.loc[:, ['article_id','raw_dataset_id','type','dataset_id_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.483908Z",
     "iopub.status.busy": "2025-09-09T18:53:21.483626Z",
     "iopub.status.idle": "2025-09-09T18:53:21.500959Z",
     "shell.execute_reply": "2025-09-09T18:53:21.500031Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.483889Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df3.drop_duplicates(ignore_index=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.514023Z",
     "iopub.status.busy": "2025-09-09T18:53:21.513648Z",
     "iopub.status.idle": "2025-09-09T18:53:21.519681Z",
     "shell.execute_reply": "2025-09-09T18:53:21.518626Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.514Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df3.dropna(ignore_index=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.520931Z",
     "iopub.status.busy": "2025-09-09T18:53:21.520599Z",
     "iopub.status.idle": "2025-09-09T18:53:21.541444Z",
     "shell.execute_reply": "2025-09-09T18:53:21.540528Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.520911Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.54338Z",
     "iopub.status.busy": "2025-09-09T18:53:21.54258Z",
     "iopub.status.idle": "2025-09-09T18:53:21.565467Z",
     "shell.execute_reply": "2025-09-09T18:53:21.564481Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.543358Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df3[dois_labels_df3.article_id == '10.3389_fevo.2023.1112519']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.566949Z",
     "iopub.status.busy": "2025-09-09T18:53:21.566593Z",
     "iopub.status.idle": "2025-09-09T18:53:21.583205Z",
     "shell.execute_reply": "2025-09-09T18:53:21.582229Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.566919Z"
    }
   },
   "outputs": [],
   "source": [
    "change_locations = dois_labels_df3[dois_labels_df3.article_id =='10.1107_s2059798322005691'].index.tolist()\n",
    "new_values_list = [\n",
    " 'https://doi.org/10.18150/VAOJLJ',\n",
    " 'https://\\ndoi.org/10.18150/R8VJ7V', \n",
    " 'https://doi.org/10.18150/\\nVAZZ2F',\n",
    " # 'https://doi.org/10.18150/T0WC49', \n",
    " 'https://doi.org/10.18150/LIENZ5',\n",
    " 'https://doi.org/10.18150/6OXPLO',\n",
    " 'https://doi.org/\\n10.18150/WEFSC9',\n",
    " # 'https://doi.org/10.18150/\\nR3BTBM',\n",
    " 'https://doi.org/10.18150/XSEXUF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.584714Z",
     "iopub.status.busy": "2025-09-09T18:53:21.58432Z",
     "iopub.status.idle": "2025-09-09T18:53:21.604939Z",
     "shell.execute_reply": "2025-09-09T18:53:21.603994Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.584683Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_labels_df3.loc[change_locations,'raw_dataset_id']= new_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.606304Z",
     "iopub.status.busy": "2025-09-09T18:53:21.606004Z",
     "iopub.status.idle": "2025-09-09T18:53:21.632965Z",
     "shell.execute_reply": "2025-09-09T18:53:21.632078Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.606278Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_dois_labels = dois_labels_df3.copy()\n",
    "\n",
    "clean_dois_labels.rename(columns={'raw_dataset_id':'dataset_id'}, inplace=True)\n",
    "\n",
    "clean_dois_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ACCESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.634187Z",
     "iopub.status.busy": "2025-09-09T18:53:21.633925Z",
     "iopub.status.idle": "2025-09-09T18:53:21.659097Z",
     "shell.execute_reply": "2025-09-09T18:53:21.6582Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.634167Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.660345Z",
     "iopub.status.busy": "2025-09-09T18:53:21.660122Z",
     "iopub.status.idle": "2025-09-09T18:53:21.827518Z",
     "shell.execute_reply": "2025-09-09T18:53:21.826693Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.660328Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_is_well_identified(dataset_id, text):\n",
    "\n",
    "    match = re.search(dataset_id, text, flags=re.IGNORECASE)\n",
    "\n",
    "    return True if match else False\n",
    "\n",
    "\n",
    "def add_acc_well_identified(article_id, dataset_id,texts=pdf_clean_train_texts):\n",
    "\n",
    "    text = texts.get(article_id)\n",
    "\n",
    "    return acc_is_well_identified(dataset_id, text)\n",
    "\n",
    "\n",
    "acc_labels_df['is_well_identified'] = acc_labels_df.apply(lambda row : add_acc_well_identified(row['article_id'], row['dataset_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.828655Z",
     "iopub.status.busy": "2025-09-09T18:53:21.828417Z",
     "iopub.status.idle": "2025-09-09T18:53:21.837166Z",
     "shell.execute_reply": "2025-09-09T18:53:21.836304Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.828637Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_labels_df[acc_labels_df.is_well_identified ==False].shape[0], acc_labels_df[acc_labels_df.is_well_identified ==False].article_id.drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.83874Z",
     "iopub.status.busy": "2025-09-09T18:53:21.838179Z",
     "iopub.status.idle": "2025-09-09T18:53:21.860816Z",
     "shell.execute_reply": "2025-09-09T18:53:21.859816Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.838706Z"
    }
   },
   "outputs": [],
   "source": [
    "not_found_accs = acc_labels_df.loc[acc_labels_df.is_well_identified ==False,:]\n",
    "\n",
    "not_found_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The article '10.1080_21645515.2023.2189598' does not contain any of the extracted mentions, so we are removing it from the training set. We are also removing the unmatched mentions from the two other articles ('10.7554_eLife.63194', '10.7554_eLife.72626').**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We search for the not found mentions in the xml files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.862575Z",
     "iopub.status.busy": "2025-09-09T18:53:21.861795Z",
     "iopub.status.idle": "2025-09-09T18:53:21.90052Z",
     "shell.execute_reply": "2025-09-09T18:53:21.899613Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.862541Z"
    }
   },
   "outputs": [],
   "source": [
    "accs_found_in_xml_tuples =  list(zip(not_found_accs[['article_id']].values.flatten().tolist(),not_found_accs[['dataset_id']].values.flatten().tolist()))\n",
    "\n",
    "\n",
    "check_list =  [re.search(t[1],xml_clean_train_texts[t[0]], flags=re.I) is not None for t in accs_found_in_xml_tuples]\n",
    "\n",
    "all(check_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the 16 mentions are present in the xml corresponding files. So, we xill keep them in the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.901923Z",
     "iopub.status.busy": "2025-09-09T18:53:21.901411Z",
     "iopub.status.idle": "2025-09-09T18:53:21.917952Z",
     "shell.execute_reply": "2025-09-09T18:53:21.916952Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.901895Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_labels_df2 = acc_labels_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.919334Z",
     "iopub.status.busy": "2025-09-09T18:53:21.919009Z",
     "iopub.status.idle": "2025-09-09T18:53:21.938045Z",
     "shell.execute_reply": "2025-09-09T18:53:21.936956Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.9193Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_labels_df2.loc[acc_labels_df2.dataset_id.isin(not_found_accs[['dataset_id']].values.flatten().tolist()),'is_well_identified'] =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.93948Z",
     "iopub.status.busy": "2025-09-09T18:53:21.939115Z",
     "iopub.status.idle": "2025-09-09T18:53:21.962081Z",
     "shell.execute_reply": "2025-09-09T18:53:21.961205Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.939445Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_acc_labels = acc_labels_df2.loc[:, clean_dois_labels.columns].copy()\n",
    "\n",
    "\n",
    "clean_acc_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final labels DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.96322Z",
     "iopub.status.busy": "2025-09-09T18:53:21.962951Z",
     "iopub.status.idle": "2025-09-09T18:53:21.981334Z",
     "shell.execute_reply": "2025-09-09T18:53:21.980502Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.9632Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_id_labels_df = pd.concat([clean_dois_labels, clean_acc_labels],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:21.982505Z",
     "iopub.status.busy": "2025-09-09T18:53:21.982251Z",
     "iopub.status.idle": "2025-09-09T18:53:21.999816Z",
     "shell.execute_reply": "2025-09-09T18:53:21.998833Z",
     "shell.execute_reply.started": "2025-09-09T18:53:21.982487Z"
    }
   },
   "outputs": [],
   "source": [
    "final_labels_df = dataset_id_labels_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.001216Z",
     "iopub.status.busy": "2025-09-09T18:53:22.000859Z",
     "iopub.status.idle": "2025-09-09T18:53:22.017679Z",
     "shell.execute_reply": "2025-09-09T18:53:22.0167Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.001195Z"
    }
   },
   "outputs": [],
   "source": [
    "#final_labels_df = final_labels_df[final_labels_df.article_id!='10.1029_2022gl100473']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.019015Z",
     "iopub.status.busy": "2025-09-09T18:53:22.018654Z",
     "iopub.status.idle": "2025-09-09T18:53:22.035725Z",
     "shell.execute_reply": "2025-09-09T18:53:22.034992Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.018983Z"
    }
   },
   "outputs": [],
   "source": [
    "final_labels_df.shape[0], dataset_id_labels_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.037076Z",
     "iopub.status.busy": "2025-09-09T18:53:22.036695Z",
     "iopub.status.idle": "2025-09-09T18:53:22.057834Z",
     "shell.execute_reply": "2025-09-09T18:53:22.056975Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.037052Z"
    }
   },
   "outputs": [],
   "source": [
    "train_article_ids = dataset_id_labels_df.article_id.drop_duplicates().values.tolist()\n",
    "\n",
    "len(train_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.059204Z",
     "iopub.status.busy": "2025-09-09T18:53:22.05878Z",
     "iopub.status.idle": "2025-09-09T18:53:22.078633Z",
     "shell.execute_reply": "2025-09-09T18:53:22.077891Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.059182Z"
    }
   },
   "outputs": [],
   "source": [
    "final_labels_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.079968Z",
     "iopub.status.busy": "2025-09-09T18:53:22.079641Z",
     "iopub.status.idle": "2025-09-09T18:53:22.095548Z",
     "shell.execute_reply": "2025-09-09T18:53:22.094752Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.07994Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_source_file(row):\n",
    "    \n",
    "    mentions_found_in_xml = dois_found_in_xml_tuples + accs_found_in_xml_tuples\n",
    "\n",
    "    df_tuple = row['article_id'],row['dataset_id']\n",
    "    xml_list = ['10.1093_nar_gkp1049',\n",
    "               '10.1021_acsomega.3c06074',\n",
    "               '10.1038_s41598-020-59839-x',\n",
    "               '10.3390_microorganisms8121872',\n",
    "              ]\n",
    "    article_id = row['article_id']\n",
    "\n",
    "    if df_tuple in mentions_found_in_xml or article_id in xml_list :\n",
    "\n",
    "        return \"xml\"\n",
    "    else:\n",
    "        return \"pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.096844Z",
     "iopub.status.busy": "2025-09-09T18:53:22.096486Z",
     "iopub.status.idle": "2025-09-09T18:53:22.116959Z",
     "shell.execute_reply": "2025-09-09T18:53:22.116119Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.096819Z"
    }
   },
   "outputs": [],
   "source": [
    "dois_found_in_xml_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.118349Z",
     "iopub.status.busy": "2025-09-09T18:53:22.117914Z",
     "iopub.status.idle": "2025-09-09T18:53:22.14107Z",
     "shell.execute_reply": "2025-09-09T18:53:22.140061Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.118322Z"
    }
   },
   "outputs": [],
   "source": [
    "final_labels_df[\"source_file\"] = final_labels_df.apply(lambda row : add_source_file(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.142265Z",
     "iopub.status.busy": "2025-09-09T18:53:22.141985Z",
     "iopub.status.idle": "2025-09-09T18:53:22.166608Z",
     "shell.execute_reply": "2025-09-09T18:53:22.165794Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.142238Z"
    }
   },
   "outputs": [],
   "source": [
    "final_labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add mention spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.167956Z",
     "iopub.status.busy": "2025-09-09T18:53:22.167623Z",
     "iopub.status.idle": "2025-09-09T18:53:22.18606Z",
     "shell.execute_reply": "2025-09-09T18:53:22.185086Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.167929Z"
    }
   },
   "outputs": [],
   "source": [
    "len(pdf_clean_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.187461Z",
     "iopub.status.busy": "2025-09-09T18:53:22.186993Z",
     "iopub.status.idle": "2025-09-09T18:53:22.202747Z",
     "shell.execute_reply": "2025-09-09T18:53:22.201841Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.187435Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_clean_texts = pdf_clean_train_texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.203941Z",
     "iopub.status.busy": "2025-09-09T18:53:22.203606Z",
     "iopub.status.idle": "2025-09-09T18:53:22.223566Z",
     "shell.execute_reply": "2025-09-09T18:53:22.222741Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.203916Z"
    }
   },
   "outputs": [],
   "source": [
    "len(pdf_clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.225Z",
     "iopub.status.busy": "2025-09-09T18:53:22.224654Z",
     "iopub.status.idle": "2025-09-09T18:53:22.241118Z",
     "shell.execute_reply": "2025-09-09T18:53:22.240335Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.224963Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_mention_spans(row):\n",
    "\n",
    "    article_id, dataset_id = row['article_id'], row['dataset_id']\n",
    "\n",
    "    if row['source_file'] ==\"pdf\":\n",
    "\n",
    "        match = re.search(re.escape(dataset_id), pdf_clean_texts.get(article_id,\"\"))\n",
    "\n",
    "        if match:\n",
    "\n",
    "            return match.span()\n",
    "    else:\n",
    "        match = re.search(re.escape(dataset_id), xml_clean_train_texts.get(article_id,\"\"))\n",
    "\n",
    "        if match:\n",
    "\n",
    "            return match.span()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.242679Z",
     "iopub.status.busy": "2025-09-09T18:53:22.242153Z",
     "iopub.status.idle": "2025-09-09T18:53:22.366829Z",
     "shell.execute_reply": "2025-09-09T18:53:22.365975Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.242651Z"
    }
   },
   "outputs": [],
   "source": [
    "final_labels_df[[\"start\",\"end\"]] = final_labels_df.apply(lambda row: add_mention_spans(row), axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.367758Z",
     "iopub.status.busy": "2025-09-09T18:53:22.367546Z",
     "iopub.status.idle": "2025-09-09T18:53:22.381031Z",
     "shell.execute_reply": "2025-09-09T18:53:22.38016Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.367741Z"
    }
   },
   "outputs": [],
   "source": [
    "final_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare training for the NER task (extract all dois and accessions from article text)\n",
    "2. Prepare the training data for the classification the task (get the type of usage of the data)\n",
    "3. Split the training data into training - validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.382366Z",
     "iopub.status.busy": "2025-09-09T18:53:22.382057Z",
     "iopub.status.idle": "2025-09-09T18:53:22.427927Z",
     "shell.execute_reply": "2025-09-09T18:53:22.426869Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.382343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility class for training data preparation\n",
    "@dataclass\n",
    "class TrainingDataPreparator:\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    class_labels: List[str] = None\n",
    "    label2id_ner: Dict[str, int] = None\n",
    "    id2label_ner: Dict[int, str] = None\n",
    "    label2id_cls: Dict[str, int] = None\n",
    "    id2label_cls: Dict[int, str] = None\n",
    "    max_seq_len: int = 512\n",
    "    ner_labels: List[str] = field(default_factory=lambda: ['O', 'B-DOI', 'I-DOI', 'B-ACC', 'I-ACC'])\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.label2id_ner = {label: idx for idx, label in enumerate(self.ner_labels)}\n",
    "        self.id2label_ner = {idx: label for label, idx in self.label2id_ner.items()}\n",
    "\n",
    "\n",
    "    def tokenize_full_text(self, text: str):\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=False\n",
    "        )\n",
    "        return encoding[\"input_ids\"], encoding[\"offset_mapping\"], self.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "\n",
    "    def get_tokens_for_mention(\n",
    "        self,\n",
    "        offsets: List[Tuple[int, int]],\n",
    "        mention_start: int,\n",
    "        mention_end: int,\n",
    "        allow_partial_overlap: bool = True\n",
    "    ) -> List[int]:\n",
    "        token_indices = []\n",
    "        for idx, (tok_start, tok_end) in enumerate(offsets):\n",
    "            if allow_partial_overlap:\n",
    "                if tok_end > mention_start and tok_start < mention_end:\n",
    "                    token_indices.append(idx)\n",
    "            else:\n",
    "                if tok_start >= mention_start and tok_end <= mention_end:\n",
    "                    token_indices.append(idx)\n",
    "        return token_indices\n",
    "\n",
    "    def extract_token_windows_around_mentions(\n",
    "        self,\n",
    "        input_ids: List[int],\n",
    "        offsets: List[Tuple[int, int]],\n",
    "        mentions: List[Dict[str, Union[str, int]]],\n",
    "        window_context_size: int = 100\n",
    "    ) -> List[Dict]:\n",
    "        windows = []\n",
    "        max_len = self.max_seq_len - 2  \n",
    "\n",
    "        for mention in mentions:\n",
    "            token_indices = self.get_tokens_for_mention(\n",
    "                offsets, mention['start'], mention['end'], allow_partial_overlap=True\n",
    "            )\n",
    "            if not token_indices:\n",
    "                continue\n",
    "\n",
    "            mention_start_idx = token_indices[0]\n",
    "            mention_end_idx = token_indices[-1]\n",
    "\n",
    "            window_start = max(0, mention_start_idx - window_context_size)\n",
    "            window_end = min(len(input_ids), mention_end_idx + 1 + window_context_size)\n",
    "\n",
    "            # Adjust window to fit max_len\n",
    "            if (window_end - window_start) > max_len:\n",
    "                excess = (window_end - window_start) - max_len\n",
    "                remove_before = min(excess // 2, window_start)\n",
    "                remove_after = excess - remove_before\n",
    "                window_start += remove_before\n",
    "                window_end -= remove_after\n",
    "\n",
    "            windows.append({\n",
    "                'input_ids': input_ids[window_start:window_end],\n",
    "                'offsets': offsets[window_start:window_end],\n",
    "                'mention': mention,\n",
    "                'window_start_idx': window_start,\n",
    "                'window_end_idx': window_end\n",
    "            })\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def biofy_labels(self, windows: List[Dict], mentions: List[Dict]) -> List[List[int]]:\n",
    "        all_labels = []\n",
    "        for w in windows:\n",
    "            labels = ['O'] * len(w['input_ids'])\n",
    "            token_spans = w['offsets']\n",
    "    \n",
    "            # Mentions available in the window\n",
    "            local_mentions = [\n",
    "                m for m in mentions\n",
    "                if m['start'] < token_spans[-1][1] and m['end'] > token_spans[0][0]\n",
    "            ]\n",
    "    \n",
    "            for mention in local_mentions:\n",
    "                token_indices = self.get_tokens_for_mention(token_spans, mention['start'], mention['end'])\n",
    "                if not token_indices:\n",
    "                    continue\n",
    "                labels[token_indices[0]] = f\"B-{mention['dataset_id_type']}\"\n",
    "                for idx in token_indices[1:]:\n",
    "                    labels[idx] = f\"I-{mention['dataset_id_type']}\"\n",
    "    \n",
    "            label_ids = [self.label2id_ner.get(label, self.label2id_ner['O']) for label in labels]\n",
    "            all_labels.append(label_ids)\n",
    "        return all_labels\n",
    "   \n",
    "    def prepare_dataset(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        text_dicts: Dict[str, str],\n",
    "        entity_col: str = \"dataset_id\",\n",
    "        type_col: str = \"dataset_id_type\",\n",
    "        article_id_col: str = \"article_id\",\n",
    "        start_col: str = \"start\",\n",
    "        end_col: str = \"end\",\n",
    "        window_context_size: int = 50\n",
    "    ) -> Dataset:\n",
    "        input_ids_all = []\n",
    "        attention_mask_all = []\n",
    "        labels_all = []\n",
    "        article_ids_all = []\n",
    "    \n",
    "        global_token_indices = set()\n",
    "        global_label_counter = Counter()\n",
    "    \n",
    "        for (article_id, source_file), group in df.groupby([article_id_col, \"source_file\"]):\n",
    "            text_dict = text_dicts.get(source_file)\n",
    "    \n",
    "            if not text_dict:\n",
    "                continue\n",
    "    \n",
    "            text = text_dict.get(article_id)\n",
    "            if not text:\n",
    "                continue\n",
    "    \n",
    "            input_ids, offsets, _ = self.tokenize_full_text(text)\n",
    "    \n",
    "            mentions = [\n",
    "                {\n",
    "                    \"start\": int(row[start_col]),\n",
    "                    \"end\": int(row[end_col]),\n",
    "                    \"dataset_id_type\": str(row[type_col])\n",
    "                }\n",
    "                for _, row in group.iterrows()\n",
    "                if row[start_col] >= 0 and row[end_col] >= 0\n",
    "            ]\n",
    "    \n",
    "            # Retrive indices of tokens present in the mentions of this article\n",
    "            all_mention_token_indices = set()\n",
    "            for mention in mentions:\n",
    "                token_indices = self.get_tokens_for_mention(offsets, mention['start'], mention['end'])\n",
    "                all_mention_token_indices.update(token_indices)\n",
    "    \n",
    "            # Add to global indices\n",
    "            global_token_indices.update(all_mention_token_indices)\n",
    "    \n",
    "            # Creating the global label for this article\n",
    "            full_labels = ['O'] * len(input_ids)\n",
    "            for mention in mentions:\n",
    "                token_indices = self.get_tokens_for_mention(offsets, mention['start'], mention['end'])\n",
    "                if not token_indices:\n",
    "                    continue\n",
    "                full_labels[token_indices[0]] = f\"B-{mention['dataset_id_type']}\"\n",
    "                for idx in token_indices[1:]:\n",
    "                    full_labels[idx] = f\"I-{mention['dataset_id_type']}\"\n",
    "    \n",
    "            # Global count of B/I labels\n",
    "            for label in full_labels:\n",
    "                if label != 'O':\n",
    "                    global_label_counter[label] += 1\n",
    "    \n",
    "            #  Splitting into windows\n",
    "            windows = self.extract_token_windows_around_mentions(\n",
    "                input_ids, offsets, mentions, window_context_size=window_context_size\n",
    "            )\n",
    "    \n",
    "            for w in windows:\n",
    "                label_window_raw = full_labels[w['window_start_idx']:w['window_end_idx']]\n",
    "                label_ids = [self.label2id_ner.get(lab, self.label2id_ner['O']) for lab in label_window_raw]\n",
    "    \n",
    "                input_ids_window = [self.tokenizer.cls_token_id] + w['input_ids'] + [self.tokenizer.sep_token_id]\n",
    "                label_window = [-100] + label_ids + [-100]\n",
    "                attention_mask = [1] * len(input_ids_window)\n",
    "    \n",
    "                # Padding\n",
    "                pad_len = self.max_seq_len - len(input_ids_window)\n",
    "                if pad_len > 0:\n",
    "                    input_ids_window += [self.tokenizer.pad_token_id] * pad_len\n",
    "                    label_window += [-100] * pad_len\n",
    "                    attention_mask += [0] * pad_len\n",
    "                else:\n",
    "                    input_ids_window = input_ids_window[:self.max_seq_len]\n",
    "                    label_window = label_window[:self.max_seq_len]\n",
    "                    attention_mask = attention_mask[:self.max_seq_len]\n",
    "    \n",
    "                input_ids_all.append(input_ids_window)\n",
    "                labels_all.append(label_window)\n",
    "                attention_mask_all.append(attention_mask)\n",
    "    \n",
    "                # Add article_id for this window\n",
    "                article_ids_all.append(article_id)\n",
    "    \n",
    "        return Dataset.from_dict({\n",
    "            \"input_ids\": input_ids_all,\n",
    "            \"attention_mask\": attention_mask_all,\n",
    "            \"labels\": labels_all,\n",
    "            \"article_id\": article_ids_all\n",
    "        })\n",
    "\n",
    "        \n",
    "    \n",
    "    def add_start_end_positions(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        text_dict: Dict[str, str],\n",
    "        entity_col: str = 'dataset_id',\n",
    "        article_id_col: str = 'article_id'\n",
    "    ) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df['start'] = -1\n",
    "        df['end'] = -1\n",
    "\n",
    "        for article_id, group in df.groupby(article_id_col):\n",
    "            text = text_dict.get(article_id, \"\")\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            used_spans = set()\n",
    "            for idx, row in group.iterrows():\n",
    "                mention = str(row[entity_col]).strip()\n",
    "                mention_escaped = re.escape(mention)\n",
    "\n",
    "                found = False\n",
    "                for match in re.finditer(mention_escaped, text):\n",
    "                    start, end = match.start(), match.end()\n",
    "                    if (start, end) not in used_spans:\n",
    "                        df.at[idx, 'start'] = start\n",
    "                        df.at[idx, 'end'] = end\n",
    "                        used_spans.add((start, end))\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "                if not found:\n",
    "                    print(f\"Mention '{mention}' not found or exhausted in article {article_id}\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER MODEL TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.429292Z",
     "iopub.status.busy": "2025-09-09T18:53:22.428996Z",
     "iopub.status.idle": "2025-09-09T18:53:22.460221Z",
     "shell.execute_reply": "2025-09-09T18:53:22.459203Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.429263Z"
    }
   },
   "outputs": [],
   "source": [
    "df_with_spans = final_labels_df.copy()\n",
    "\n",
    "df_with_spans = df_with_spans[df_with_spans.article_id != '10.1029_2022gl100473']\n",
    "\n",
    "text_dicts = {\"pdf\":pdf_clean_texts,\"xml\":xml_clean_train_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.46169Z",
     "iopub.status.busy": "2025-09-09T18:53:22.461325Z",
     "iopub.status.idle": "2025-09-09T18:53:22.480953Z",
     "shell.execute_reply": "2025-09-09T18:53:22.480149Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.461656Z"
    }
   },
   "outputs": [],
   "source": [
    "df_with_spans.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:22.482169Z",
     "iopub.status.busy": "2025-09-09T18:53:22.481812Z",
     "iopub.status.idle": "2025-09-09T18:53:45.101692Z",
     "shell.execute_reply": "2025-09-09T18:53:45.100841Z",
     "shell.execute_reply.started": "2025-09-09T18:53:22.482109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Load  SciBERT Tokenizer and Model\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "preparator = TrainingDataPreparator(tokenizer=ner_tokenizer)\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(preparator.ner_labels),\n",
    "    id2label=preparator.id2label_ner,\n",
    "    label2id=preparator.label2id_ner\n",
    ")\n",
    "\n",
    "# 2. Add start and end of mentions\n",
    "# df_with_spans = preparator.add_start_end_positions(df_mentions, text_dict)\n",
    "\n",
    "# 3. Prépare the dataset\n",
    "full_dataset = preparator.prepare_dataset(df_with_spans, text_dicts)\n",
    "\n",
    "# 4. Split train/validation\n",
    "# Ensure the 'article_id' column exists in the dataset\n",
    "assert \"article_id\" in full_dataset.column_names, \"'article_id' column must be present in the dataset\"\n",
    "\n",
    "# Extract article IDs as a NumPy array\n",
    "article_ids = np.array(full_dataset[\"article_id\"])\n",
    "\n",
    "# Instantiate the GroupShuffleSplit with fixed random seed\n",
    "gss = GroupShuffleSplit(test_size=0.25, random_state=42)\n",
    "\n",
    "# Perform the split using article_id as grouping variable\n",
    "train_indices, val_indices = next(gss.split(np.arange(len(article_ids)), groups=article_ids))\n",
    "\n",
    "# Select subsets from the full dataset using the computed indices\n",
    "ner_train_dataset = full_dataset.select(train_indices)\n",
    "ner_val_dataset = full_dataset.select(val_indices)\n",
    "\n",
    "# # 4. Split train/validation\n",
    "# dataset_split = full_dataset.train_test_split(test_size=0.25, seed=42)\n",
    "# ner_train_dataset = dataset_split[\"train\"]\n",
    "# ner_val_dataset = dataset_split[\"test\"]\n",
    "\n",
    "# 5. Define metrics\n",
    "\n",
    "def ner_compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[preparator.id2label_ner[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[preparator.id2label_ner[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "                  for pred, label in zip(preds, labels)]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
    "    }\n",
    "# 6. Entraînement\n",
    "ner_output_dir = MODELS_DIR/'ner_model'\n",
    "ner_training_args = TrainingArguments(\n",
    "    output_dir = ner_output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\", \n",
    ")\n",
    "\n",
    "ner_trainer = Trainer(\n",
    "    model=ner_model,\n",
    "    args=ner_training_args,\n",
    "    train_dataset=ner_train_dataset,\n",
    "    eval_dataset=ner_val_dataset,\n",
    "    processing_class=ner_tokenizer,\n",
    "    compute_metrics=ner_compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:45.102867Z",
     "iopub.status.busy": "2025-09-09T18:53:45.102578Z",
     "iopub.status.idle": "2025-09-09T18:53:45.108219Z",
     "shell.execute_reply": "2025-09-09T18:53:45.107339Z",
     "shell.execute_reply.started": "2025-09-09T18:53:45.102842Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ner_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:45.109639Z",
     "iopub.status.busy": "2025-09-09T18:53:45.109206Z",
     "iopub.status.idle": "2025-09-09T18:53:45.14962Z",
     "shell.execute_reply": "2025-09-09T18:53:45.148828Z",
     "shell.execute_reply.started": "2025-09-09T18:53:45.109609Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ner_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:45.150928Z",
     "iopub.status.busy": "2025-09-09T18:53:45.150556Z",
     "iopub.status.idle": "2025-09-09T18:53:45.334583Z",
     "shell.execute_reply": "2025-09-09T18:53:45.333698Z",
     "shell.execute_reply.started": "2025-09-09T18:53:45.150903Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_labels = [label for sequence in full_dataset['labels'] for label in sequence if label != -100]\n",
    "Counter(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T18:53:45.3364Z",
     "iopub.status.busy": "2025-09-09T18:53:45.335445Z",
     "iopub.status.idle": "2025-09-09T19:04:48.988044Z",
     "shell.execute_reply": "2025-09-09T19:04:48.987398Z",
     "shell.execute_reply.started": "2025-09-09T18:53:45.336371Z"
    }
   },
   "outputs": [],
   "source": [
    "ner_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:48.988895Z",
     "iopub.status.busy": "2025-09-09T19:04:48.988662Z",
     "iopub.status.idle": "2025-09-09T19:04:51.892153Z",
     "shell.execute_reply": "2025-09-09T19:04:51.891532Z",
     "shell.execute_reply.started": "2025-09-09T19:04:48.988878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on validation data\n",
    "eval_results = ner_trainer.predict(ner_trainer.eval_dataset)\n",
    "\n",
    "predictions = eval_results.predictions  \n",
    "labels = eval_results.label_ids         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:51.893144Z",
     "iopub.status.busy": "2025-09-09T19:04:51.892924Z",
     "iopub.status.idle": "2025-09-09T19:04:51.984497Z",
     "shell.execute_reply": "2025-09-09T19:04:51.983817Z",
     "shell.execute_reply.started": "2025-09-09T19:04:51.893127Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics_no_O(p, id2label):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Convert ids to labels, ignoring -100 padding tokens\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[id2label[pred] for pred, l in zip(pred_seq, label) if l != -100]\n",
    "                  for pred_seq, label in zip(preds, labels)]\n",
    "\n",
    "    filtered_labels = []\n",
    "    filtered_preds = []\n",
    "    for label_seq, pred_seq in zip(true_labels, true_preds):\n",
    "        new_label_seq = []\n",
    "        new_pred_seq = []\n",
    "        for l, p in zip(label_seq, pred_seq):\n",
    "            if l != \"O\":\n",
    "                new_label_seq.append(l)\n",
    "                new_pred_seq.append(p)\n",
    "        if new_label_seq:\n",
    "            filtered_labels.append(new_label_seq)\n",
    "            filtered_preds.append(new_pred_seq)\n",
    "\n",
    "    # Get string report \n",
    "    report_str = classification_report(filtered_labels, filtered_preds)\n",
    "    print(report_str)\n",
    "\n",
    "    # Compute numeric metrics\n",
    "    prec = precision_score(filtered_labels, filtered_preds)\n",
    "    rec = recall_score(filtered_labels, filtered_preds)\n",
    "    f1 = f1_score(filtered_labels, filtered_preds)\n",
    "\n",
    "    return {\n",
    "        \"precision_no_O\": prec,\n",
    "        \"recall_no_O\": rec,\n",
    "        \"f1_no_O\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "results = compute_metrics_no_O((predictions, labels), preparator.id2label_ner)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:51.985422Z",
     "iopub.status.busy": "2025-09-09T19:04:51.985217Z",
     "iopub.status.idle": "2025-09-09T19:04:55.293908Z",
     "shell.execute_reply": "2025-09-09T19:04:55.292975Z",
     "shell.execute_reply.started": "2025-09-09T19:04:51.985405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix on validation data\n",
    "# metric = evaluate.load(\"seqeval\")\n",
    "predictions_output = ner_trainer.predict(ner_val_dataset)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "\n",
    "\n",
    "preds = np.argmax(logits, axis=2)\n",
    "\n",
    "\n",
    "true_labels = [\n",
    "    [preparator.id2label_ner[label] for label in label_seq if label != -100]\n",
    "    for label_seq in labels\n",
    "]\n",
    "true_preds = [\n",
    "    [preparator.id2label_ner[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "    for pred_seq, label_seq in zip(preds, labels)\n",
    "]\n",
    "\n",
    "\n",
    "flat_preds = [p for seq in true_preds for p in seq]\n",
    "flat_labels = [l for seq in true_labels for l in seq]\n",
    "\n",
    "filtered_preds = [p for p, l in zip(flat_preds, flat_labels) if l != 'O']\n",
    "filtered_labels = [l for l in flat_labels if l != 'O']\n",
    "\n",
    "labels_unique = sorted(list(set(filtered_labels + filtered_preds)))\n",
    "\n",
    "cm = confusion_matrix(filtered_labels, filtered_preds, labels=labels_unique)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels_unique, yticklabels=labels_unique, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Actual values\")\n",
    "plt.title(\"Confusion Matrix (without 'O')\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POST-PROCESSING AND NER INFERENCE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building heuristic rules to distinguish dataset mentions from articles references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:55.29492Z",
     "iopub.status.busy": "2025-09-09T19:04:55.294675Z",
     "iopub.status.idle": "2025-09-09T19:04:55.301498Z",
     "shell.execute_reply": "2025-09-09T19:04:55.300627Z",
     "shell.execute_reply.started": "2025-09-09T19:04:55.294903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extraction of prefixes and suffixes representing data repositories from the training set\n",
    "def get_dois_prefixes_suffixes(raw_doi: str) -> Tuple[str,str]:\n",
    "    if not raw_doi:\n",
    "        return \"\"\n",
    "    doi = raw_doi.strip().replace('\\n', '').replace(' ', '')\n",
    "    doi = doi.replace('https://dx/','')\n",
    "    doi = re.sub(r'^(doi:|DOI:)', '', doi, flags=re.IGNORECASE)\n",
    "    doi = re.sub(r'^(https?://)?(dx\\.)?doi\\.org/', '', doi, flags=re.IGNORECASE)\n",
    "\n",
    "    if len(doi.split('/')) == 2:\n",
    "        \n",
    "        prefix = doi.split('/')[0]\n",
    "        \n",
    "        if len(doi.split('/')[1].split('.'))>=2:\n",
    "        \n",
    "            suffix = doi.split('/')[1].split('.')[0]\n",
    "    \n",
    "            if prefix == '10.17632':\n",
    "                \n",
    "                suffix =''\n",
    "            return  prefix, suffix  \n",
    "\n",
    "        else:\n",
    "            \n",
    "            return prefix, ''\n",
    "\n",
    "\n",
    "            return prefix, doi.split('/')[1].replace('.'+last,'')\n",
    "    else:\n",
    "        prefix, suffix = doi.split('/')[0],doi.split('/')[1]\n",
    "\n",
    "    return  prefix, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:55.302562Z",
     "iopub.status.busy": "2025-09-09T19:04:55.30224Z",
     "iopub.status.idle": "2025-09-09T19:04:55.329637Z",
     "shell.execute_reply": "2025-09-09T19:04:55.329085Z",
     "shell.execute_reply.started": "2025-09-09T19:04:55.302536Z"
    }
   },
   "outputs": [],
   "source": [
    "dois = final_labels_df[final_labels_df.dataset_id_type=='DOI'].dataset_id.values.tolist()\n",
    "\n",
    "pre_suffixes = [get_dois_prefixes_suffixes(doi) for doi in dois]\n",
    "\n",
    "prefixes, suffixes =[],[]\n",
    "\n",
    "for pre_suffix in pre_suffixes:\n",
    "\n",
    "    pre, suf = pre_suffix\n",
    "\n",
    "    if pre not in prefixes:\n",
    "        \n",
    "        if  pre!='' and pre not in ['http:','https:']:\n",
    "            \n",
    "            prefixes.append(pre)\n",
    "        \n",
    "    if suf not in suffixes:\n",
    "        if suf !='':\n",
    "            suffixes.append(suf)\n",
    "            \n",
    "len(prefixes), len(suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By cleaning the prefixes and suffixes, and grouping them, we obtain:**\n",
    "['10.5281','10.5061','10.7937','10.23728', '10.6084','10.34740', '10.17632', '10.7910',\n",
    " '10.5072','10.15125','10.5255', '10.5257', '10.1594', '10.17605', '10.3886', '10.25337',\n",
    " '10.34739', '10.5878', '10.15468', '10.5067', '10.5066', '10.5291', '10.17638', '10.2305',\n",
    " '10.7289','10.17862', '10.11583', '10.23642', '10.5285', '10.5441',  '10.17882', '10.25349', \n",
    " '10.6075', '10.6073', '10.17863', '10.11588', '10.21942', '10.25377', '10.3334', '10.13020', \n",
    " '10.25326', '10.25921', '10.34973', '10.15482', '10.18150', '10.15131', '10.4121', '10.5256', \n",
    " '10.13012', '10.25387', '10.25386', '10.5518', '10.6078', '10.15485', '10.25422', '10.22033',\n",
    " '10.6096', '10.18434', '10.24381', '10.7291',\n",
    " 'zenodo', 'dryad', 'figshare', 'mendeley', 'dataverse', 'ICPSR', 'ukda-sn', 'pangaea', 'snd', \n",
    " 'ILL-DATA', 'datacat', 'CDIAC', 'IUCN.UK', 'GPM', 'IMMERGDF', 'OTG', 'sussex', 'CHEMBL', 'TCIA',\n",
    " 'cranfield', 'dtu', 'usn', 'K9', 'cranﬁeld', 'cranfield','pasta', 'CAM', 'data', 'DVN', 'uva', 'dl',\n",
    " 'USDA.ADC', 'shef.data', 'ICPSR', 'datacat.liverpool.ac.uk', 'f1000research', 'g3',\n",
    " 'genetics', 'MODIS', 'azu', 'ESGF', 'Suborbital', 'AEROCLO', 'cds',\n",
    "]\n",
    "**I also added a few suffixes not found in the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:55.33065Z",
     "iopub.status.busy": "2025-09-09T19:04:55.330375Z",
     "iopub.status.idle": "2025-09-09T19:04:55.347983Z",
     "shell.execute_reply": "2025-09-09T19:04:55.347223Z",
     "shell.execute_reply.started": "2025-09-09T19:04:55.330632Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_alpha_prefix(acc: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the alphabetic prefix from an accession string.\n",
    "    Keeps underscores and hyphens that appear before the first digit.\n",
    "    Discards prefixes shorter than 2 characters.\n",
    "\n",
    "    Examples:\n",
    "        - 'GSE12345'       -> 'GSE'\n",
    "        - 'EPI_ISL_291131' -> 'EPI_ISL_'\n",
    "        - 'E-PROT-100'     -> 'E-PROT-'\n",
    "        - 'A1'             -> None (too short)\n",
    "\n",
    "    Args:\n",
    "        acc (str): Accession string\n",
    "\n",
    "    Returns:\n",
    "        str or None: Extracted prefix, or None if not valid\n",
    "    \"\"\"\n",
    "    match = re.match(r'^([A-Za-z_-]+(?:[_-][A-Za-z]+)*[_-]*)\\d+', acc)\n",
    "    if match:\n",
    "        prefix = match.group(1)\n",
    "        if len(prefix.strip(\"_-\")) >= 2:\n",
    "            return prefix\n",
    "    return None\n",
    "\n",
    "\n",
    "def collect_acc_prefixes(acc_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts a list of unique alphabetic prefixes from a list of accession strings.\n",
    "\n",
    "    Args:\n",
    "        acc_list (List[str]): List of accession strings.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of unique alphabetic prefixes (no duplicates).\n",
    "    \"\"\"\n",
    "    prefixes = {\n",
    "        extract_alpha_prefix(acc) for acc in acc_list\n",
    "        if extract_alpha_prefix(acc) is not None\n",
    "    }\n",
    "    return prefixes\n",
    "\n",
    "accs = acc_labels_df.dataset_id.tolist()\n",
    "acc_prefixes = collect_acc_prefixes(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of ACC prefixes** :['BX', 'CAB', 'CHEMBL', 'CP', 'CVCL_','CVCL_D',\n",
    " 'CVCL_KS', 'E-GEOD-', 'E-PROT-', 'EMPIAR-', 'ENSBTAG', 'ENSMMUT', 'ENSOARG',\n",
    " 'EPI', 'EPI_ISL_', 'ERR', 'GSE', 'HPA', 'IPR', 'KX', 'MODEL', 'NC_',\n",
    " 'NM_', 'PF', 'PRJNA', 'PXD', 'SAMN', 'SRP', 'SRR', 'SRX', 'STH', 'rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:55.349224Z",
     "iopub.status.busy": "2025-09-09T19:04:55.348902Z",
     "iopub.status.idle": "2025-09-09T19:04:55.389588Z",
     "shell.execute_reply": "2025-09-09T19:04:55.388928Z",
     "shell.execute_reply.started": "2025-09-09T19:04:55.349206Z"
    }
   },
   "outputs": [],
   "source": [
    "#A class that filters the most plausible data mentions.\n",
    "class DatasetMentionFilter:\n",
    "    \"\"\"\n",
    "    Filters extracted mentions to retain only those likely referring to datasets.\n",
    "    Applies confidence threshold, deduplication, context extraction, and format-specific heuristics\n",
    "    (DOI or ACC), including support for detection of mentions embedded in tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, confidence_threshold: float = 0.90, \n",
    "                file_format=\"pdf\",\n",
    "                dataset_indicators: List[str] = None,\n",
    "                dataset_keywords: List[str] = None,\n",
    "                context_window_chars: int = 300,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the filter.\n",
    "\n",
    "        :param confidence_threshold: Minimum confidence required to keep a mention.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.file_format = file_format.lower()\n",
    "        self.context_window_chars = context_window_chars\n",
    "\n",
    "        self.acc_valid_prefixes = [\n",
    "        'BX', 'CAB', 'CHEMBL', 'CP', 'CVCL_', 'CVCL_D', 'CVCL_KS', 'E-GEOD-', 'E-PROT-', 'EMPIAR-',\n",
    "        'ENSBTAG', 'ENSMMUT', 'ENSOARG', 'EPI', 'EPI_ISL_', 'ERR', 'GSE', 'HPA', 'IPR', 'KX',\"GCA\",\n",
    "        'MODEL', 'NC_', 'NM_', 'PF','PRJNA','PXD', 'SAMN', 'SRP', 'SRR', 'SRX', 'STH', 'rs',\n",
    "        'KT','MN','JN', 'JX', 'FM', 'KC','KP','AY', 'AF','AM','PNQY','FM','EU','KM','GU','GQ','CP',\n",
    "        'FJ','HQ','DQ','JQ','MG','MSV','PRJEB','KT','MF','MH','K0','HGNC','LT','LC','Be',\n",
    "    ]\n",
    "\n",
    "        # Known DOI prefixes and keywords that indicate datasets\n",
    "        self.dataset_indicators = dataset_indicators or ['10.5281','10.5061','10.7937','10.23728', '10.6084','10.34740', '10.17632', '10.7910',\n",
    "             '10.5072','10.15125','10.5255', '10.5257', '10.1594', '10.17605', '10.3886', '10.25337',\n",
    "             '10.34739', '10.5878', '10.15468', '10.5067', '10.5066', '10.5291', '10.17638', '10.2305',\n",
    "             '10.7289','10.17862', '10.11583', '10.23642', '10.5285', '10.5441',  '10.17882', '10.25349', \n",
    "             '10.6075', '10.6073', '10.17863', '10.11588', '10.21942', '10.25377', '10.3334', '10.13020', \n",
    "             '10.25326', '10.25921', '10.34973', '10.15482', '10.18150', '10.15131', '10.4121', '10.5256', \n",
    "             '10.13012', '10.25387', '10.25386', '10.5518', '10.6078', '10.15485', '10.25422', '10.22033',\n",
    "             '10.6096', '10.18434', '10.24381', '10.7291', '10.5065','zenodo', 'dryad', 'figshare', 'mendeley', \n",
    "             'dataverse', 'ICPSR', 'ukda-sn', 'pangaea', 'snd', 'ILL-DATA', 'datacat', 'CDIAC', 'IUCN.UK', \n",
    "             'GPM', 'IMMERGDF', 'OTG', 'sussex', 'CHEMBL', 'TCIA', 'cranfield', 'dtu', 'usn', 'K9', \n",
    "             'cranﬁeld', 'pasta', 'CAM', 'data', 'DVN', 'uva', 'dl','USDA.ADC', 'shef.data', 'ICPSR', \n",
    "             'datacat.liverpool.ac.uk',  'MODIS', 'azu', 'ESGF',  'Suborbital', 'AEROCLO', 'cds',\n",
    "             'ccdc','uhhfdm', 'hepdata', 'wdcc', 'm9', 'rodare', '10.5517', '10.5061', '10.5441',\n",
    "             '10.25592','10.17182', '10.1594', '10.6084', '10.11583',\n",
    "             '10.15468', '10.5878','10.14278', '10.26093','10.13155', #'f1000research', 'g3','genetics',\n",
    "        ]\n",
    "\n",
    "        self.dataset_keywords = dataset_keywords or [\n",
    "             \"data availability statement\",\"These data are provided\", \n",
    "            \"deposition number\",'data accessibility',\"Associated data for this publication\",\n",
    "            \"data available for download from\", \"deposition numbers\",\n",
    "            \"data downloaded from\",\"Data Repository\", \"Data used in this\",  \"ccdc\",\n",
    "            \"Data referring to\",\"Data deposition\", \"Acession numbers\",\"Accession no\",\n",
    "            \"PDB references\",\"The following data sets\",\"The following data set\",\n",
    "            \"The following datasets\",\"The following dataset\",'accession number',\n",
    "            \"contains the supplementary crystallographic data for this paper\",\n",
    "            'Protein Data Bank','GenBank','NCBI','Accession no.',\n",
    "            'International Nucleotide Sequence Database Collaboration','INSDC','DDBJ','ENA',\n",
    "            'Global Initiative on Sharing All Influenza Data (GISAID)','GISAID','(NCBI) database',\n",
    "            'submitted to the National Center for Biotechnology Information',\n",
    "            'Data associated with this paper have been deposited','Imagery considered in the analysis',\n",
    "            'sample data','Dryad. Dataset',\n",
    "          \n",
    "        ]\n",
    "\n",
    "        self.funding_keywords = [\n",
    "            \"funding\", \n",
    "            \"grant\", \n",
    "            \"grant number\", \n",
    "            \"grant/award\", \n",
    "            \"award number\",\n",
    "            \"funding information\",\n",
    "            'This research was funded by',\n",
    "            'This work was supported financially by',\n",
    "            'is supported by',\n",
    "                ]\n",
    "\n",
    "    def is_likely_funding_mention(self, context: str, mention_text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the sentence containing the mention_text suggests it's part of funding or grant information.\n",
    "    \n",
    "        :param context: Full context text\n",
    "        :param mention_text: The exact mention (used to find the sentence containing it)\n",
    "        :return: True if likely funding-related\n",
    "        \"\"\"\n",
    "    \n",
    "        # Normalize\n",
    "        context = context.strip()\n",
    "        mention_text = mention_text.strip()\n",
    "    \n",
    "        if not mention_text:\n",
    "            return False\n",
    "    \n",
    "        # Tokenize context into sentences\n",
    "        sentences = sent_tokenize(context)\n",
    "    \n",
    "        # Find the sentence that contains the mention\n",
    "        target_sentence = \"\"\n",
    "        for sentence in sentences:\n",
    "            if mention_text in sentence:\n",
    "                target_sentence = sentence.lower()\n",
    "                break\n",
    "    \n",
    "        if not target_sentence:\n",
    "            return False  # mention not found in any sentence\n",
    "    \n",
    "        # Search for keywords in the sentence only\n",
    "        for keyword in self.funding_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            if \" \" in keyword:\n",
    "                if keyword in target_sentence:\n",
    "                    return True\n",
    "            else:\n",
    "                pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "                if re.search(pattern, target_sentence):\n",
    "                    return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def has_valid_acc_prefix(self,acc: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the accession string starts with one of the known valid prefixes.\n",
    "    \n",
    "        Args:\n",
    "            acc (str): Accession string to check.\n",
    "    \n",
    "        Returns:\n",
    "            bool: True if the accession starts with a valid prefix, False otherwise.\n",
    "        \"\"\"\n",
    "        acc = acc.strip()\n",
    "        return any(acc.startswith(prefix) for prefix in self.acc_valid_prefixes)\n",
    "\n",
    "        \n",
    "    def get_canonical_doi(self,mention: str) ->str:\n",
    "        \n",
    "        if not mention:\n",
    "            return \"\"\n",
    "        doi = mention.strip().replace('\\n', '').replace(' ', '')\n",
    "        doi = doi.replace('https://dx/','')\n",
    "        doi = re.sub(r'^(doi:|DOI:)', '', doi, flags=re.IGNORECASE)\n",
    "        doi = re.sub(r'^(https?://)?(dx\\.)?doi\\.org/', '', doi, flags=re.IGNORECASE)\n",
    "        \n",
    "        return doi\n",
    "\n",
    "    def is_valid_doi_format(self,doi: str) -> bool:\n",
    "        \n",
    "        doi_pattern = r'(10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+)'\n",
    "        \n",
    "        doi_clean = self.get_canonical_doi(doi)\n",
    "        \n",
    "        return re.fullmatch(doi_pattern, doi_clean, flags=re.IGNORECASE) is not None\n",
    "    \n",
    "    def is_valid_acc_format(self,acc: str) -> bool:\n",
    "    \n",
    "        acc_clean = acc.strip()\n",
    "        pattern = re.compile(\n",
    "            r'^[A-Z0-9_.\\-]{4,}$'                      \n",
    "            r'|'\n",
    "            r'^[A-Z0-9_.\\-]{4,}\\s*-\\s*[A-Z0-9_.\\-]{4,}$' \n",
    "            ,\n",
    "            flags=re.IGNORECASE \n",
    "        )\n",
    "        if acc_clean.isalpha():\n",
    "            \n",
    "            return False\n",
    "\n",
    "        if len(acc_clean)<4:\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        if acc_clean[0].isdigit():\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        return re.fullmatch(pattern, acc_clean) is not None\n",
    "\n",
    "\n",
    "    def is_article_doi(self, mention: str, article_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the mention corresponds to the article's own DOI.\n",
    "\n",
    "        :param mention: Mention string\n",
    "        :param article_id: ID of the article (underscores instead of slashes)\n",
    "        :return: True if mention refers to article DOI\n",
    "        \"\"\"\n",
    "        # pattern = article_id.replace(\"_\", \"/\")\n",
    "        pattern = article_id.split('_')[0]\n",
    "        return pattern in mention \n",
    "\n",
    "    def has_dataset_indicator(self, mention: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks whether the mention contains a known dataset-related DOI prefix or name.\n",
    "\n",
    "        :param mention: Mention string\n",
    "        :return: True if known dataset indicator is found\n",
    "        \"\"\"\n",
    "        mention = mention.lower()\n",
    "        return any(indicator.lower() in mention for indicator in self.dataset_indicators)\n",
    "\n",
    "\n",
    "    def contains_keywords(self, context: str,mention_start:int) -> bool:\n",
    "        context = context.lower()\n",
    "    \n",
    "        for keyword in self.dataset_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            if \" \" in keyword:\n",
    "                kw_index = context.find(keyword)\n",
    "            else:\n",
    "                pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "                match = re.search(pattern, context)\n",
    "                kw_index = match.start() if match else -1\n",
    "    \n",
    "            if kw_index != -1 and mention_start > kw_index:\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "    def deduplicate_extractions(self, extractions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Removes duplicate mentions based on normalized mention text.\n",
    "\n",
    "        :param extractions: List of mention dictionaries\n",
    "        :return: Deduplicated list\n",
    "        \"\"\"\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for m in extractions:\n",
    "            key = m[\"mention\"].strip().lower()\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                deduped.append(m)\n",
    "        return deduped\n",
    "\n",
    "    def is_truncated_mention(self, mention: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detects obviously incomplete or broken mentions, such as partial DOI strings.\n",
    "        Returns True only if the mention is clearly invalid or unusable.\n",
    "        \"\"\"\n",
    "        mention = mention.strip().lower()\n",
    "    \n",
    "        # Known invalid or partial forms\n",
    "        known_invalid = [\n",
    "            \"https\", \"http\", \"doi\", \"doi.org\", \"dx.doi.org\",\n",
    "            \"https://\", \"http://\", \"https://doi.org\", \"http://doi.org\",\n",
    "            \"https://doi.org/\", \"http://doi.org/\",\n",
    "            \"https://dx.doi.org\", \"http://dx.doi.org\",\n",
    "            \"https://dx.doi.org/\", \"http://dx.doi.org/\",\n",
    "            \"https://doi.org/10.\", \"http://doi.org/10.\",\n",
    "            \"https://dx.doi.org/10.\", \"http://dx.doi.org/10.\"\n",
    "        ]\n",
    "        if mention in known_invalid:\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        if re.fullmatch(r\"10\\.\\d{4,9}/\", mention):\n",
    "            \n",
    "            return True\n",
    "\n",
    "        if len(self.get_canonical_doi(mention)) <12:\n",
    "\n",
    "            return True\n",
    "    \n",
    "        # Check for DOI-like prefixes but too short to be valid (≤25 chars)\n",
    "        doi_prefixes = [\n",
    "            \"https://doi.org/10.\", \"http://doi.org/10.\",\n",
    "            \"https://dx.doi.org/10.\", \"http://dx.doi.org/10.\"\n",
    "        ]\n",
    "        if any(mention.startswith(prefix) and len(mention) <= 25 for prefix in doi_prefixes):\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def remove_truncated_mentions(self, mentions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Removes truncated or partial dataset mentions based solely on textual comparison.\n",
    "    \n",
    "        This function compares all mentions and removes any mention that is a strict substring\n",
    "        of a longer one (e.g., \"zenodo.12345\" will be removed if \"10.5281/zenodo.12345\" is present).\n",
    "        It does not rely on character offsets, making it robust to duplicate mentions found in\n",
    "        different parts of the text.\n",
    "    \n",
    "        Args:\n",
    "            mentions (List[Dict]): A list of mention dictionaries, each containing at least a\n",
    "                                   \"mention\" key with the extracted text.\n",
    "    \n",
    "        Returns:\n",
    "            List[Dict]: A filtered list of mentions with truncated duplicates removed.\n",
    "        \"\"\"\n",
    "        mention_texts = [m[\"mention\"].strip().lower() for m in mentions]\n",
    "        to_remove = set()\n",
    "    \n",
    "        for i, m1 in enumerate(mention_texts):\n",
    "            for j, m2 in enumerate(mention_texts):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if m1 != m2 and m1 in m2 and len(m1) < len(m2):\n",
    "                    to_remove.add(i)\n",
    "                    break\n",
    "    \n",
    "        return [m for i, m in enumerate(mentions) if i not in to_remove]\n",
    "\n",
    "\n",
    "    def get_context_window_around_chars(self, text: str, start: int, end: int) -> str:\n",
    "        \"\"\"\n",
    "        Extracts ±N characters around a mention.\n",
    "\n",
    "        :param text: Full article text\n",
    "        :param start: Start position of the mention\n",
    "        :param end: End position of the mention\n",
    "        :param window: Number of characters before and after to include\n",
    "        :return: Context string\n",
    "        \"\"\"\n",
    "        left = max(0, start - self.context_window_chars)\n",
    "        right = min(len(text), end + self.context_window_chars)\n",
    "        return text[left:right]      \n",
    "\n",
    "    def extract_context(self,text: str, start: int, end: int) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the sentence(s) containing the mention, and optionally includes one or two sentences\n",
    "        before and one after, based on rules:\n",
    "        \n",
    "        - If the mention spans multiple sentences, include all of them.\n",
    "        - Always include one sentence before and one after (unless close to the edge).\n",
    "        - If the mention starts close to the beginning of a sentence, include two before.\n",
    "        - If the mention ends near the end of a sentence, don't add one after.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Define a local window around the mention to improve performance\n",
    "        window_size = 1500\n",
    "        window_start = max(0, start - window_size)\n",
    "        window_end = min(len(text), end + window_size)\n",
    "        local_text = text[window_start:window_end]\n",
    "    \n",
    "        # Tokenize the local window into sentences\n",
    "        sentences = sent_tokenize(local_text)\n",
    "    \n",
    "        # Compute mention's position relative to the local window\n",
    "        local_start = start - window_start\n",
    "        local_end = end - window_start\n",
    "        mention_span = set(range(local_start, local_end))\n",
    "    \n",
    "        # Track character spans of each sentence within the local window\n",
    "        spans = []\n",
    "        current_pos = 0\n",
    "        for sentence in sentences:\n",
    "            s_start = local_text.find(sentence, current_pos)\n",
    "            s_end = s_start + len(sentence)\n",
    "            spans.append((s_start, s_end))\n",
    "            current_pos = s_end\n",
    "    \n",
    "        # Identify sentences that overlap with the mention\n",
    "        overlapping_indices = []\n",
    "        for i, (s_start, s_end) in enumerate(spans):\n",
    "            if mention_span & set(range(s_start, s_end)):\n",
    "                overlapping_indices.append(i)\n",
    "    \n",
    "        # Fallback: if no overlap is found, return a basic character window\n",
    "        if not overlapping_indices:\n",
    "            return local_text[local_start - 100: local_end + 100]\n",
    "    \n",
    "        first_idx = overlapping_indices[0]\n",
    "        last_idx = overlapping_indices[-1]\n",
    "    \n",
    "        # Default: include one sentence before and one after\n",
    "        start_idx = max(0, first_idx - 1)\n",
    "        end_idx = min(len(sentences), last_idx + 2)\n",
    "    \n",
    "        # Rule: if the mention starts very close to the beginning of a sentence, include two before\n",
    "        if abs(local_start - spans[first_idx][0]) < 40:\n",
    "            start_idx = max(0, first_idx - 2)\n",
    "    \n",
    "        # Rule: if the mention ends very close to the end of a sentence, skip adding one after\n",
    "        if abs(local_end - spans[last_idx][1]) < 5:\n",
    "            end_idx = last_idx + 1\n",
    "    \n",
    "        # Join the selected sentences as the final context\n",
    "        context = \" \".join(sentences[start_idx:end_idx])\n",
    "        return context.strip()\n",
    "    \n",
    "    \n",
    "    def filter_mentions(\n",
    "        self,\n",
    "        mentions_dict: Dict[str, List[Dict]],\n",
    "        #texts: Dict[str, str]\n",
    "        texts_dict\n",
    "    ) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Filters mentions for all articles.\n",
    "\n",
    "        :param mentions_dict: Dictionary with article_id as key and list of mention dicts as value\n",
    "        :param texts: Dictionary with article_id as key and article text as value\n",
    "        :return: Filtered dictionary with same structure\n",
    "        \"\"\"\n",
    "        result = defaultdict(dict)\n",
    "\n",
    "        for article_id, mentions in mentions_dict.items():\n",
    "\n",
    "            mentions = self.remove_truncated_mentions(mentions)\n",
    "\n",
    "            #  Extract dois\n",
    "            doi_mentions = [m for m in mentions if m[\"mention_format\"] == \"DOI\"]\n",
    "\n",
    "            # Remove truncated dois\n",
    "            doi_mentions = [m for m in doi_mentions if not self.is_truncated_mention(m[\"mention\"])]\n",
    "\n",
    "            # Validate doi format \n",
    "            validated_dois = [m for m in doi_mentions if self.is_valid_doi_format(m[\"mention\"])]\n",
    "\n",
    "            cleaned_dois = []\n",
    "            for m in validated_dois:\n",
    "                source_file = m.get('source_file','')\n",
    "                if not source_file:\n",
    "                    continue\n",
    "                texts = texts_dict.get(source_file,{})\n",
    "                if not texts:\n",
    "                    continue\n",
    "                text = texts.get(article_id, \"\")\n",
    "                if not text:\n",
    "                    continue\n",
    "                    \n",
    "                if self.is_article_doi(m[\"mention\"], article_id):\n",
    "                    continue\n",
    "                m[\"context\"] = self.extract_context(text, m[\"start\"], m[\"end\"])\n",
    "                \n",
    "                cleaned_dois.append(m)\n",
    "\n",
    "            cleaned_dois = [ m for m in cleaned_dois if m['context']]\n",
    "            \n",
    "            # Filter dois by confidence threshold\n",
    "            cleaned_dois = [m for m in cleaned_dois if m[\"confidence\"] >= self.confidence_threshold]\n",
    "        \n",
    "\n",
    "            # Filter relevant DOI mentions\n",
    "            selected_dois = [m for m in cleaned_dois if self.contains_keywords(m[\"context\"],m['start'])]\n",
    "            remaining_dois = [m for m in cleaned_dois if m not in selected_dois]\n",
    "            selected_dois += [m for m in remaining_dois if self.has_dataset_indicator(m[\"mention\"])]\n",
    " \n",
    "\n",
    "            # Deduplicate dois \n",
    "            # selected_dois = self.deduplicate_extractions(selected_dois)\n",
    "\n",
    "            # Extract accessions\n",
    "            acc_mentions = [m for m in mentions if m[\"mention_format\"] == \"ACC\"]\n",
    "            # Validate acc format\n",
    "            validated_accs = [m for m in acc_mentions if self.is_valid_acc_format(m[\"mention\"])]\n",
    "\n",
    "            # Filter accessions by confidence threshold\n",
    "            validated_accs = [m for m in validated_accs if m[\"confidence\"] >= self.confidence_threshold]\n",
    "\n",
    "            # Get context for accessions\n",
    "            cleaned_accs = []\n",
    "            for m in validated_accs:\n",
    "                source_file = m.get('source_file','')\n",
    "                if not source_file:\n",
    "                    continue\n",
    "                texts = texts_dict.get(source_file,{})\n",
    "                if not texts:\n",
    "                    continue\n",
    "                text = texts.get(article_id, \"\")\n",
    "                if not text:\n",
    "                    continue\n",
    "                m[\"context\"] = self.extract_context(text, m[\"start\"], m[\"end\"])\n",
    "                # if not self.is_likely_funding_mention(m[\"context\"]):\n",
    "                #     cleaned_accs.append(m)\n",
    "            \n",
    "            cleaned_accs = [ m for m in validated_accs if m['context']]\n",
    "            selected_accs = [m for m in cleaned_accs if self.contains_keywords(m[\"context\"],m['start'])]\n",
    "            remaining_accs = [m for m in cleaned_accs if m not in selected_accs]\n",
    "            selected_accs += [m for m in remaining_accs if self.has_valid_acc_prefix(m[\"mention\"])]\n",
    "            selected_accs = [m for m in selected_accs if not self.is_likely_funding_mention(m[\"context\"],m['mention'])]\n",
    "            # Filter relevant ACC mentions \n",
    "            # selected_accs = [\n",
    "            #                 {**m, \"mention\": self.remove_ccdc_from_acc(m[\"mention\"])}\n",
    "            #                 for m in selected_accs\n",
    "            #                 ]\n",
    "            # Remove duplicated accessions                \n",
    "            # selected_accs = self.deduplicate_extractions(selected_accs)\n",
    "            \n",
    "            # Remove temporary 'context' fields\n",
    "            for m in selected_dois:\n",
    "                m.pop(\"context\", None)\n",
    "            for m in selected_accs:\n",
    "                m.pop(\"context\", None)\n",
    "\n",
    "            selected_mentions = selected_dois + selected_accs\n",
    "\n",
    "\n",
    "            result[article_id] = selected_mentions\n",
    "\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION OF DATA USAGE TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:55.390797Z",
     "iopub.status.busy": "2025-09-09T19:04:55.390493Z",
     "iopub.status.idle": "2025-09-09T19:04:55.418609Z",
     "shell.execute_reply": "2025-09-09T19:04:55.417989Z",
     "shell.execute_reply.started": "2025-09-09T19:04:55.390753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility class to prepare classification dataset\n",
    "class ClassificationDataPreparator:\n",
    "    def __init__(self, tokenizer, max_seq_len: int = 512,max_ctx_length =1600):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_ctx_length = max_ctx_length\n",
    "        self.label2id = {\n",
    "            \"primary\": 0,\n",
    "            \"secondary\": 1,\n",
    "            # \"missing\": 2,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def tokenize_classification_dataset(self, dataset: Dataset) -> Dataset:\n",
    "        return dataset.map(\n",
    "            lambda x: self.tokenizer(\n",
    "                x[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_seq_len\n",
    "            ),\n",
    "            batched=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_mention_context(self, text: str, mention_start: int, mention_end: int) -> str:\n",
    "        \n",
    "        \"\"\"\n",
    "        Extracts the most relevant context around a mention for classification by:\n",
    "        - Extracting a standard context window around the mention.\n",
    "        - If the standard context is too long, switching to a table-aware context extraction.\n",
    "        - Otherwise, enriching the context with relevant data sections.\n",
    "        - Removing trailing sections starting from 'References' or 'Acknowledgement' that appear\n",
    "          after the mention text in the context.\n",
    "    \n",
    "        Parameters:\n",
    "        - text (str): The full document text.\n",
    "        - mention_start (int): The character start index of the mention in the text.\n",
    "        - mention_end (int): The character end index of the mention in the text.\n",
    "    \n",
    "        Returns:\n",
    "        - str: A cleaned and focused context string surrounding the mention.\n",
    "        \"\"\"\n",
    "    \n",
    "        standard_context = self.extract_standard_context(text, mention_start, mention_end)\n",
    "        \n",
    "        if len(standard_context) >= self.max_ctx_length:\n",
    "            context = self.extract_table_context(text, mention_start, mention_end,standard_context)\n",
    "        else:\n",
    "            context = self.extract_data_section_context(text, standard_context)\n",
    "            context = self.remove_number_only_lines(context)\n",
    "        \n",
    "        # Determine mention_text from the original text and mention positions\n",
    "        mention_text = text[mention_start:mention_end]\n",
    "        \n",
    "        # Clean unwanted trailing sections after the mention\n",
    "        #context = self.remove_references_acknowledgements_section(context, mention_text)\n",
    "        \n",
    "        return context\n",
    "\n",
    "    def extract_data_section_context(self,text: str, standard_context: str) -> str:\n",
    "        \"\"\"\n",
    "        Extracts additional context from the data-related section using the last match of known patterns.\n",
    "        Only includes sentences not already present in the standard_context. Uses a local window for efficiency.\n",
    "    \n",
    "        :param text: Full document text\n",
    "        :param standard_context: Previously extracted context (e.g. near a mention)\n",
    "        :return: Combined enriched context string\n",
    "        \"\"\"\n",
    "    \n",
    "        # Compile common patterns used in data availability sections\n",
    "        pattern = re.compile(\n",
    "            r'Data accessibility|Data availability|DNA Deposition|Data Deposition|'\n",
    "            r'Data acquisition|Data preparation|Data and software availability|'\n",
    "            r'DATA ARCHIVING STATEMENT|DATA AVAILABILITY STATEMENT|Data and metadata repository|DATA AND RESOURCES',\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "    \n",
    "        # First, check if pattern is already in the standard context\n",
    "        match = pattern.search(standard_context)\n",
    "        if match:\n",
    "            start, _ = match.span()\n",
    "            return standard_context[start:].strip()\n",
    "    \n",
    "        # Find all matches in the full text\n",
    "        matches = list(re.finditer(pattern, text))\n",
    "        if not matches:\n",
    "            return standard_context.strip()\n",
    "    \n",
    "        # Get the last match position\n",
    "        last_match = matches[-1]\n",
    "        match_start = last_match.start()\n",
    "    \n",
    "        # === Local window optimization ===\n",
    "        window_size = 1500\n",
    "        window_start = max(0, match_start - window_size)\n",
    "        window_end = min(len(text), match_start + window_size)\n",
    "        local_text = text[window_start:window_end]\n",
    "    \n",
    "        # Tokenize sentences only in the local window\n",
    "        sentences = sent_tokenize(local_text)\n",
    "        standard_sentences = set(sent_tokenize(standard_context))\n",
    "    \n",
    "        # Adjust match index relative to local_text\n",
    "        relative_match_start = match_start - window_start\n",
    "    \n",
    "        # Find sentence in local_text that contains the pattern\n",
    "        current_pos = 0\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence_start = local_text.find(sentence, current_pos)\n",
    "            sentence_end = sentence_start + len(sentence)\n",
    "            current_pos = sentence_end\n",
    "    \n",
    "            if sentence_start <= relative_match_start < sentence_end:\n",
    "                selected_sentences = sentences[i : i + 3]  # sentence with match + next 2\n",
    "                unique_sentences = [s for s in selected_sentences if s not in standard_sentences]\n",
    "    \n",
    "                if not unique_sentences:\n",
    "                    return standard_context.strip()\n",
    "    \n",
    "                return (\" \".join(unique_sentences) + \" \" + standard_context).strip()\n",
    "    \n",
    "        # Fallback: nothing matched cleanly\n",
    "        return standard_context.strip()\n",
    "\n",
    "    def extract_standard_context(self,text: str, start: int, end: int) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the sentence(s) containing the mention, and optionally includes one or two sentences\n",
    "        before and one after, based on rules:\n",
    "        \n",
    "        - If the mention spans multiple sentences, include all of them.\n",
    "        - Always include one sentence before and one after (unless close to the edge).\n",
    "        - If the mention starts close to the beginning of a sentence, include two before.\n",
    "        - If the mention ends near the end of a sentence, don't add one after.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Define a local window around the mention to improve performance\n",
    "        window_size = 1500\n",
    "        window_start = max(0, start - window_size)\n",
    "        window_end = min(len(text), end + window_size)\n",
    "        local_text = text[window_start:window_end]\n",
    "    \n",
    "        # Tokenize the local window into sentences\n",
    "        sentences = sent_tokenize(local_text)\n",
    "    \n",
    "        # Compute mention's position relative to the local window\n",
    "        local_start = start - window_start\n",
    "        local_end = end - window_start\n",
    "        mention_span = set(range(local_start, local_end))\n",
    "    \n",
    "        # Track character spans of each sentence within the local window\n",
    "        spans = []\n",
    "        current_pos = 0\n",
    "        for sentence in sentences:\n",
    "            s_start = local_text.find(sentence, current_pos)\n",
    "            s_end = s_start + len(sentence)\n",
    "            spans.append((s_start, s_end))\n",
    "            current_pos = s_end\n",
    "    \n",
    "        # Identify sentences that overlap with the mention\n",
    "        overlapping_indices = []\n",
    "        for i, (s_start, s_end) in enumerate(spans):\n",
    "            if mention_span & set(range(s_start, s_end)):\n",
    "                overlapping_indices.append(i)\n",
    "    \n",
    "        # Fallback: if no overlap is found, return a basic character window\n",
    "        if not overlapping_indices:\n",
    "            return local_text[local_start - 100: local_end + 100]\n",
    "    \n",
    "        first_idx = overlapping_indices[0]\n",
    "        last_idx = overlapping_indices[-1]\n",
    "    \n",
    "        # Default: include one sentence before and one after\n",
    "        start_idx = max(0, first_idx - 1)\n",
    "        end_idx = min(len(sentences), last_idx + 2)\n",
    "    \n",
    "        # Rule: if the mention starts very close to the beginning of a sentence, include two before\n",
    "        if abs(local_start - spans[first_idx][0]) < 40:\n",
    "            start_idx = max(0, first_idx - 2)\n",
    "    \n",
    "        # Rule: if the mention ends very close to the end of a sentence, skip adding one after\n",
    "        if abs(local_end - spans[last_idx][1]) < 5:\n",
    "            end_idx = last_idx + 1\n",
    "    \n",
    "        # Join the selected sentences as the final context\n",
    "        context = \" \".join(sentences[start_idx:end_idx])\n",
    "        return context.strip()\n",
    "\n",
    "    def extract_table_context(self, text: str, mention_start: int, mention_end: int, standard_context) -> str:\n",
    "        \"\"\"\n",
    "        Extracts context for mentions in tables by:\n",
    "        - Finding the last match of data-related section patterns\n",
    "        - Taking the sentence that contains the match and the next two\n",
    "        - Adjusting the local window around the mention:\n",
    "            * If a section is found: 300 chars before the end of the mention\n",
    "            * Otherwise: 1600 chars before the mention start\n",
    "    \n",
    "        :param text: Full article text\n",
    "        :param mention_start: Character index where the mention starts\n",
    "        :param mention_end: Character index where the mention ends\n",
    "        :return: Combined context string\n",
    "        \"\"\"\n",
    "    \n",
    "        pattern = re.compile(\n",
    "            r'Data accessibility|Data availability|DNA Deposition|Data Deposition|'\n",
    "            r'Data acquisition|Data preparation|Data and software availability|'\n",
    "            r'DATA ARCHIVING STATEMENT|DATA AVAILABILITY STATEMENT|Data and metadata repository|DATA AND RESOURCES',\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "    \n",
    "    \n",
    "        # Find all pattern matches and take the last one\n",
    "        matches = list(re.finditer(pattern, text))\n",
    "        global_context = \"\"\n",
    "    \n",
    "        if matches:\n",
    "            last_match = matches[-1]\n",
    "            match_start = last_match.start()\n",
    "    \n",
    "            # === Local window optimization ===\n",
    "            window_size = 1500\n",
    "            window_start = max(0, match_start - window_size)\n",
    "            window_end = min(len(text), match_start + window_size)\n",
    "            local_text = text[window_start:window_end]\n",
    "    \n",
    "            # Tokenize only the local window\n",
    "            sentences = sent_tokenize(local_text)\n",
    "            relative_match_start = match_start - window_start\n",
    "    \n",
    "            current_pos = 0\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                sentence_start = local_text.find(sentence, current_pos)\n",
    "                sentence_end = sentence_start + len(sentence)\n",
    "                current_pos = sentence_end\n",
    "    \n",
    "                if sentence_start <= relative_match_start < sentence_end:\n",
    "                    # Get sentence with match + next two\n",
    "                    selected = sentences[i: i + 4]\n",
    "                    global_context = \" \".join(selected).strip()\n",
    "                    break\n",
    "    \n",
    "        # === Define local context around the mention ===\n",
    "        if global_context:\n",
    "            local_context = text[max(0, mention_end - 300): mention_end].strip()\n",
    "        else:\n",
    "            # Get mention_text from global text\n",
    "            mention_text = text[mention_start:mention_end].strip()\n",
    "        \n",
    "            # Find position of mention in standard_context\n",
    "            local_pos = standard_context.find(mention_text)\n",
    "            if local_pos == -1:\n",
    "                # Fallback if mention_text not found: return truncated standard_context\n",
    "                local_context = standard_context[:1600].strip()\n",
    "            else:\n",
    "                # Slice standard_context with local window\n",
    "                window_start = max(0, local_pos - 2000)\n",
    "                window_end = min(len(standard_context), local_pos + len(mention_text))\n",
    "                local_context = standard_context[window_start:window_end].strip()\n",
    "    \n",
    "        # === Combine contexts ===\n",
    "        if global_context:\n",
    "            match = pattern.search(global_context)\n",
    "            if match:\n",
    "                start, _ = match.span()\n",
    "                global_context = global_context[start:]\n",
    "            return f\"{global_context} {local_context}\".strip()\n",
    "        else:\n",
    "            return local_context\n",
    "\n",
    "\n",
    "    def remove_number_only_lines(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes lines that contain only numbers or numbers with spaces (e.g., line/page numbers).\n",
    "        \n",
    "        :param text: Raw input text from article\n",
    "        :return: Cleaned text without number-only lines\n",
    "        \"\"\"\n",
    "        cleaned_lines = []\n",
    "        for line in text.splitlines():\n",
    "            # Strip line to check if it only contains digits and optional spaces\n",
    "            if not re.fullmatch(r'\\s*\\d[\\d\\s]*\\s*', line):\n",
    "                cleaned_lines.append(line)\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "    def prepare_classification_dataset(\n",
    "        self,\n",
    "        df_mentions,\n",
    "        text_dicts: Dict[str, Dict[str, str]],\n",
    "    ) -> Dataset:\n",
    "        \"\"\"Prepares a Hugging Face dataset for classification by extracting\n",
    "        cleaned sentence contexts for each mention, while removing duplicates (article_id, text).\"\"\"\n",
    "    \n",
    "        texts = []\n",
    "        labels = []\n",
    "        article_ids = []\n",
    "    \n",
    "        seen_pairs = set()  # To track (article_id, context) combinations already added\n",
    "    \n",
    "        # Group mentions by article and source file for performance\n",
    "        for (article_id, source_file), group in df_mentions.groupby([\"article_id\", \"source_file\"]):\n",
    "            text = text_dicts.get(source_file, {}).get(article_id, None)\n",
    "            if not text:\n",
    "                continue\n",
    "    \n",
    "            for _, row in group.iterrows():\n",
    "                start = row['start']\n",
    "                end = row['end']\n",
    "                label_str = row['type'].lower()\n",
    "    \n",
    "                if label_str not in self.label2id:\n",
    "                    continue\n",
    "    \n",
    "                context = self.extract_mention_context(text, start, end)\n",
    "                if not context:\n",
    "                    continue\n",
    "                    \n",
    "                context = context.strip()\n",
    "    \n",
    "                pair = (article_id, context)\n",
    "                if pair in seen_pairs:\n",
    "                    continue\n",
    "    \n",
    "                seen_pairs.add(pair)\n",
    "                texts.append(context.strip())\n",
    "                labels.append(self.label2id[label_str])\n",
    "                article_ids.append(article_id)\n",
    "    \n",
    "        return Dataset.from_dict({\n",
    "            \"text\": texts,\n",
    "            \"label\": labels,\n",
    "            \"article_id\": article_ids\n",
    "        })\n",
    "\n",
    "    def stratified_group_split(self, dataset, label_col=\"label\", group_col=\"article_id\", test_size=0.25, seed=44):\n",
    "        df = dataset.to_pandas()\n",
    "    \n",
    "        n_splits = round(1 / test_size)\n",
    "        sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        \n",
    "        y = df[label_col]\n",
    "        groups = df[group_col]\n",
    "        \n",
    "        for train_idx, val_idx in sgkf.split(df, y, groups):\n",
    "            train_dataset = dataset.select(train_idx)\n",
    "            val_dataset = dataset.select(val_idx)\n",
    "            \n",
    "            # Optional: show class distribution\n",
    "            from collections import Counter\n",
    "            train_labels = train_dataset[label_col]\n",
    "            val_labels = val_dataset[label_col]\n",
    "            \n",
    "            print(f\"Train class distribution: {Counter(train_labels)}\")\n",
    "            print(f\"Val class distribution: {Counter(val_labels)}\")\n",
    "            \n",
    "            return train_dataset, val_dataset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:55.419632Z",
     "iopub.status.busy": "2025-09-09T19:04:55.419336Z",
     "iopub.status.idle": "2025-09-09T19:04:55.489011Z",
     "shell.execute_reply": "2025-09-09T19:04:55.488217Z",
     "shell.execute_reply.started": "2025-09-09T19:04:55.419607Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Tokenizer and data preparation class definition\n",
    "\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "cls_preparator = ClassificationDataPreparator(tokenizer=cls_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:55.490076Z",
     "iopub.status.busy": "2025-09-09T19:04:55.489866Z",
     "iopub.status.idle": "2025-09-09T19:04:57.227665Z",
     "shell.execute_reply": "2025-09-09T19:04:57.22675Z",
     "shell.execute_reply.started": "2025-09-09T19:04:55.49006Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Prepare raw data\n",
    "df_mentions = final_labels_df.copy()\n",
    "\n",
    "#df_mentions = df_mentions[df_mentions.article_id!='10.1080_21645515.2023.2189598']\n",
    "\n",
    "df_mentions['type'] = df_mentions['type'].str.lower()\n",
    "\n",
    "\n",
    "raw_dataset = cls_preparator.prepare_classification_dataset(df_mentions, text_dicts)\n",
    "# Définir les classes\n",
    "class_label = ClassLabel(names=[\"primary\", \"secondary\"])\n",
    "\n",
    "# Convertir le dataset\n",
    "raw_dataset = raw_dataset.cast_column(\"label\", class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:57.228975Z",
     "iopub.status.busy": "2025-09-09T19:04:57.228622Z",
     "iopub.status.idle": "2025-09-09T19:04:57.232805Z",
     "shell.execute_reply": "2025-09-09T19:04:57.232057Z",
     "shell.execute_reply.started": "2025-09-09T19:04:57.228947Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Tokenization\n",
    "tokenized_dataset = cls_preparator.tokenize_classification_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:04:59.327687Z",
     "iopub.status.busy": "2025-09-09T19:04:59.327207Z",
     "iopub.status.idle": "2025-09-09T19:05:00.463584Z",
     "shell.execute_reply": "2025-09-09T19:05:00.462707Z",
     "shell.execute_reply.started": "2025-09-09T19:04:59.327668Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Split par article\n",
    "cls_train_dataset, cls_val_dataset = cls_preparator.stratified_group_split(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:05:00.465009Z",
     "iopub.status.busy": "2025-09-09T19:05:00.464656Z",
     "iopub.status.idle": "2025-09-09T19:05:01.698382Z",
     "shell.execute_reply": "2025-09-09T19:05:01.697416Z",
     "shell.execute_reply.started": "2025-09-09T19:05:00.464983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Metric for Evaluation\n",
    "def cls_compute_metrics(pred):\n",
    "\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_pred=labels, y_true=preds, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:05:01.699734Z",
     "iopub.status.busy": "2025-09-09T19:05:01.699341Z",
     "iopub.status.idle": "2025-09-09T19:05:02.531812Z",
     "shell.execute_reply": "2025-09-09T19:05:02.531152Z",
     "shell.execute_reply.started": "2025-09-09T19:05:01.699706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"primary\", 1: \"secondary\"},\n",
    "    label2id={\"primary\": 0, \"secondary\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:05:02.532718Z",
     "iopub.status.busy": "2025-09-09T19:05:02.532524Z",
     "iopub.status.idle": "2025-09-09T19:05:06.777974Z",
     "shell.execute_reply": "2025-09-09T19:05:06.777111Z",
     "shell.execute_reply.started": "2025-09-09T19:05:02.532702Z"
    }
   },
   "outputs": [],
   "source": [
    "# TrainingArguments and Trainer Setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=cls_tokenizer)\n",
    "\n",
    "cls_output_dir = MODELS_DIR/\"type_classifier\"\n",
    "cls_training_args = TrainingArguments(\n",
    "    output_dir=cls_output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\", \n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "cls_trainer = Trainer(\n",
    "    model=cls_model,\n",
    "    args=cls_training_args,\n",
    "    train_dataset=cls_train_dataset,\n",
    "    eval_dataset=cls_val_dataset,\n",
    "    processing_class=cls_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=cls_compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:05:06.779567Z",
     "iopub.status.busy": "2025-09-09T19:05:06.779284Z",
     "iopub.status.idle": "2025-09-09T19:10:31.859283Z",
     "shell.execute_reply": "2025-09-09T19:10:31.858383Z",
     "shell.execute_reply.started": "2025-09-09T19:05:06.779541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate\n",
    "cls_trainer.train()\n",
    "eval_results = cls_trainer.evaluate()\n",
    "print(f\"Validation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:10:31.860794Z",
     "iopub.status.busy": "2025-09-09T19:10:31.860428Z",
     "iopub.status.idle": "2025-09-09T19:10:31.866442Z",
     "shell.execute_reply": "2025-09-09T19:10:31.865453Z",
     "shell.execute_reply.started": "2025-09-09T19:10:31.860747Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, normalize=False, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true' if normalize else None)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:10:31.867726Z",
     "iopub.status.busy": "2025-09-09T19:10:31.867428Z",
     "iopub.status.idle": "2025-09-09T19:10:33.716377Z",
     "shell.execute_reply": "2025-09-09T19:10:33.715629Z",
     "shell.execute_reply.started": "2025-09-09T19:10:31.867708Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = cls_trainer.predict(cls_val_dataset)\n",
    "y_pred = predictions.predictions.argmax(axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, class_names=[\"Primary\", \"Secondary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE ON TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:10:33.789707Z",
     "iopub.status.busy": "2025-09-09T19:10:33.789422Z",
     "iopub.status.idle": "2025-09-09T19:10:33.820025Z",
     "shell.execute_reply": "2025-09-09T19:10:33.819201Z",
     "shell.execute_reply.started": "2025-09-09T19:10:33.789679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility class to extract, filter and classify data reference mentions in articles\n",
    "class MentionExtractorAndClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_dicts,\n",
    "        ner_model: PreTrainedModel,\n",
    "        ner_tokenizer: PreTrainedTokenizer,\n",
    "        classifier_model: PreTrainedModel,\n",
    "        classifier_tokenizer: PreTrainedTokenizer,\n",
    "        mention_filter: DatasetMentionFilter,\n",
    "        cls_preparator: ClassificationDataPreparator,\n",
    "        classification_window: int = 300,\n",
    "        strides: List[int] = [312],#[256, 312,412,484],\n",
    "        device: Optional[str] = None,\n",
    "        batch_size: int = 8,\n",
    "    ):\n",
    "        self.ner_model = ner_model\n",
    "        self.ner_tokenizer = ner_tokenizer\n",
    "        self.classifier_model = classifier_model\n",
    "        self.classifier_tokenizer = classifier_tokenizer\n",
    "        self.mention_filter = mention_filter\n",
    "        self.classification_window = classification_window\n",
    "        self.strides = strides\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        self.text_dicts = text_dicts\n",
    "        self.cls_preparator = cls_preparator\n",
    "\n",
    "   \n",
    "    def ner_inference_with_sliding(self, text: str,source_file:str) -> List[Dict]:\n",
    "        all_mentions = []\n",
    "        for stride in self.strides:\n",
    "            mentions = self._ner_inference(text, stride=stride,source_file = source_file)\n",
    "            all_mentions.extend(mentions)\n",
    "\n",
    "        # Supprimer les doublons exacts\n",
    "        seen = set()\n",
    "        unique_mentions = []\n",
    "        for m in all_mentions:\n",
    "            key = (m['start'], m['end'], m['mention_format'])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_mentions.append(m)\n",
    "        return unique_mentions\n",
    "\n",
    "    def _ner_inference(\n",
    "        self,\n",
    "        text: str,\n",
    "        stride: int,\n",
    "        max_length: int = 512,\n",
    "        source_file: str = \"pdf\",\n",
    "    ) -> List[Dict]:\n",
    "        model = self.ner_model\n",
    "        tokenizer = self.ner_tokenizer\n",
    "        device = self.device\n",
    "        batch_size = self.batch_size\n",
    "    \n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "    \n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=False,\n",
    "            truncation=False,\n",
    "            padding=False\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        id2label = model.config.id2label\n",
    "    \n",
    "        all_preds = [None] * len(input_ids)\n",
    "        all_scores = [0.0] * len(input_ids)\n",
    "        all_offsets = [None] * len(input_ids)\n",
    "    \n",
    "        def get_label_at(idx):\n",
    "            if 0 <= idx < len(all_preds):\n",
    "                pred_id = all_preds[idx]\n",
    "                return id2label[pred_id] if pred_id is not None else \"O\"\n",
    "            return \"O\"\n",
    "    \n",
    "        if len(input_ids) <= max_length - 2:\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits[0][1:-1]\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                pred_ids = torch.argmax(probs, dim=-1).cpu().tolist()\n",
    "                confidences = torch.max(probs, dim=-1).values.cpu().tolist()\n",
    "    \n",
    "            all_preds = pred_ids\n",
    "            all_scores = confidences\n",
    "            all_offsets = offset_mapping[1:-1]\n",
    "    \n",
    "        else:\n",
    "            window_size = max_length - 2\n",
    "            start = 0\n",
    "            while start < len(input_ids):\n",
    "                end = min(start + window_size, len(input_ids))\n",
    "                chunk_ids = input_ids[start:end]\n",
    "                chunk_offsets = offsets[start:end]\n",
    "    \n",
    "                input_chunk = [tokenizer.cls_token_id] + chunk_ids + [tokenizer.sep_token_id]\n",
    "                attention_mask = [1] * len(input_chunk)\n",
    "    \n",
    "                pad_len = max_length - len(input_chunk)\n",
    "                input_chunk += [tokenizer.pad_token_id] * pad_len\n",
    "                attention_mask += [0] * pad_len\n",
    "    \n",
    "                input_tensor = {\n",
    "                    \"input_ids\": torch.tensor([input_chunk], device=device),\n",
    "                    \"attention_mask\": torch.tensor([attention_mask], device=device)\n",
    "                }\n",
    "    \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**input_tensor)\n",
    "                    logits = outputs.logits[0][1:1 + len(chunk_ids)]\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    pred_ids = torch.argmax(probs, dim=-1).cpu().tolist()\n",
    "                    confidences = torch.max(probs, dim=-1).values.cpu().tolist()\n",
    "    \n",
    "                for i, idx in enumerate(range(start, end)):\n",
    "                    if confidences[i] > all_scores[idx]:\n",
    "                        all_preds[idx] = pred_ids[i]\n",
    "                        all_scores[idx] = confidences[i]\n",
    "                        all_offsets[idx] = chunk_offsets[i]\n",
    "    \n",
    "                new_start = start + stride\n",
    "                while (\n",
    "                    new_start < len(input_ids) and\n",
    "                    new_start < start + window_size and\n",
    "                    get_label_at(new_start).startswith(\"I-\")\n",
    "                ):\n",
    "                    new_start += 1\n",
    "                if new_start == start:\n",
    "                    break\n",
    "                start = new_start\n",
    "    \n",
    "        # Mention extraction\n",
    "        mentions = []\n",
    "        current_label = None\n",
    "        current_score_sum = 0.0\n",
    "        token_count = 0\n",
    "        start_char = None\n",
    "        end_char = None\n",
    "    \n",
    "        for pred_id, offset, score in zip(all_preds, all_offsets, all_scores):\n",
    "            if offset is None:\n",
    "                continue\n",
    "            start, end = map(int, offset)\n",
    "            if start == end:\n",
    "                continue\n",
    "    \n",
    "            label = id2label[pred_id]\n",
    "    \n",
    "            if label.startswith(\"B-\"):\n",
    "                # Save previous mention\n",
    "                if current_label:\n",
    "                    mentions.append({\n",
    "                        \"mention\": text[start_char:end_char],\n",
    "                        \"mention_format\": current_label,\n",
    "                        \"start\": start_char,\n",
    "                        \"end\": end_char,\n",
    "                        \"confidence\": current_score_sum / token_count,\n",
    "                        \"source_file\": source_file,\n",
    "                    })\n",
    "                # Start new mention\n",
    "                current_label = label[2:]\n",
    "                start_char = start\n",
    "                end_char = end\n",
    "                current_score_sum = score\n",
    "                token_count = 1\n",
    "    \n",
    "            elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "                end_char = end\n",
    "                current_score_sum += score\n",
    "                token_count += 1\n",
    "    \n",
    "            else:\n",
    "                # End current mention if exists\n",
    "                if current_label:\n",
    "                    mentions.append({\n",
    "                        \"mention\": text[start_char:end_char],\n",
    "                        \"mention_format\": current_label,\n",
    "                        \"start\": start_char,\n",
    "                        \"end\": end_char,\n",
    "                        \"confidence\": current_score_sum / token_count,\n",
    "                        \"source_file\": source_file,\n",
    "                    })\n",
    "                current_label = None\n",
    "                start_char = None\n",
    "                end_char = None\n",
    "                current_score_sum = 0.0\n",
    "                token_count = 0\n",
    "    \n",
    "        if current_label:\n",
    "            mentions.append({\n",
    "                \"mention\": text[start_char:end_char],\n",
    "                \"mention_format\": current_label,\n",
    "                \"start\": start_char,\n",
    "                \"end\": end_char,\n",
    "                \"confidence\": current_score_sum / token_count,\n",
    "                \"source_file\": source_file,\n",
    "            })\n",
    "    \n",
    "        # Optional: merge overlapping mentions with same type\n",
    "        merged = []\n",
    "        for m in mentions:\n",
    "            if merged and merged[-1]['mention_format'] == m['mention_format'] and m['start'] <= merged[-1]['end']:\n",
    "                overlap = merged[-1]['end'] - m['start']\n",
    "                suffix = m['mention'][overlap:] if overlap < len(m['mention']) else \"\"\n",
    "                merged[-1]['mention'] += suffix\n",
    "                merged[-1]['end'] = m['end']\n",
    "                merged[-1]['confidence'] = (merged[-1]['confidence'] + m['confidence']) / 2\n",
    "            else:\n",
    "                merged.append(m)\n",
    "    \n",
    "        return merged\n",
    "\n",
    "\n",
    "\n",
    "    def classify_mentions(self, article_id: str, mentions: List[Dict]) -> Dict:\n",
    "        self.classifier_model.eval()\n",
    "        self.classifier_model.to(self.device)\n",
    "    \n",
    "        for m in mentions:\n",
    "            start = m[\"start\"]\n",
    "            end = m['end']\n",
    "            source_file = m['source_file']\n",
    "    \n",
    "            text_dict = self.text_dicts.get(source_file, {})\n",
    "            text = text_dict.get(article_id, \"\")\n",
    "    \n",
    "            if not text:\n",
    "                m[\"type\"] = 2\n",
    "                continue\n",
    "    \n",
    "            context = self.cls_preparator.extract_mention_context(text, start, end)\n",
    "            if not context:\n",
    "                m[\"type\"] = 2\n",
    "                m['cls_confidence'] = 0.0\n",
    "                m['context'] = None\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            inputs = self.classifier_tokenizer(\n",
    "                            context,\n",
    "                            return_tensors=\"pt\",\n",
    "                            truncation=True,\n",
    "                            max_length=512,\n",
    "                            padding=\"max_length\"\n",
    "                        ).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.classifier_model(**inputs)\n",
    "                logits = getattr(output, \"logits\", output[0])\n",
    "                prob = torch.softmax(logits, dim=-1)\n",
    "                pred = torch.argmax(prob, dim=-1).item()\n",
    "                confidence = prob[0, pred].item()\n",
    "                \n",
    "            m['context'] = context\n",
    "            m[\"type\"] = pred  # 0 = Primary, 1 = Secondary\n",
    "            m['cls_confidence'] = confidence\n",
    "            m['standard_context'] = self.cls_preparator.extract_standard_context(text, start, end)\n",
    "        return mentions\n",
    "\n",
    "    def process_articles(self, articles_dicts) -> Dict[str, List[Dict]]:\n",
    "        \n",
    "        all_mentions = defaultdict(list)\n",
    "        pdf_articles = articles_dicts.get(\"pdf\", {})\n",
    "        xml_articles = articles_dicts.get(\"xml\", {})\n",
    "    \n",
    "        for article_id, text in pdf_articles.items():\n",
    "            try:\n",
    "                mentions = self.ner_inference_with_sliding(text=text, source_file=\"pdf\")\n",
    "                if mentions:\n",
    "                    all_mentions[article_id].extend(mentions)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"NER PDF failed for {article_id}: {e}\")\n",
    "    \n",
    "        for article_id, text in xml_articles.items():\n",
    "            try:\n",
    "                mentions = self.ner_inference_with_sliding(text=text, source_file=\"xml\")\n",
    "                if mentions:\n",
    "                    all_mentions[article_id].extend(mentions)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"NER XML failed for {article_id}: {e}\")\n",
    "    \n",
    "        filtered_mentions = self.mention_filter.filter_mentions(all_mentions, articles_dicts)\n",
    "    \n",
    "        classified_mentions = {}\n",
    "        for article_id, mentions in filtered_mentions.items():\n",
    "            if mentions:\n",
    "                classified = self.classify_mentions(article_id, mentions)\n",
    "                classified_mentions[article_id] = classified\n",
    "    \n",
    "        return classified_mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:10:33.821823Z",
     "iopub.status.busy": "2025-09-09T19:10:33.821532Z",
     "iopub.status.idle": "2025-09-09T19:10:33.849456Z",
     "shell.execute_reply": "2025-09-09T19:10:33.848614Z",
     "shell.execute_reply.started": "2025-09-09T19:10:33.821791Z"
    }
   },
   "outputs": [],
   "source": [
    "# A complete pipeline to extract, filter, classify, normalize and export data reference mentions from articles\n",
    "class FindDataReferences:\n",
    "    \"\"\"\n",
    "    Pipeline class to extract and classify dataset references from a set of scientific articles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mention_extractor_classifier: MentionExtractorAndClassifier,\n",
    "        #extractor,   #Instance of ExtractTextFromArticles\n",
    "        mention_filter,  # Instance of DatasetMentionFilter\n",
    "        text_dicts,\n",
    "        missing_ids,\n",
    "    ):\n",
    "        self.mention_extractor_classifier = mention_extractor_classifier\n",
    "        #self.extractor = extractor\n",
    "        self.mention_filter = mention_filter\n",
    "        self.text_dicts  = text_dicts\n",
    "        self.missing_ids = missing_ids\n",
    "\n",
    "        # Logger\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        if not self.logger.hasHandlers():\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def normalize_article_doi(self,doi):\n",
    "    \n",
    "        doi = doi.replace('/','_').replace(' ','').lower()\n",
    "    \n",
    "        return doi\n",
    "        \n",
    "    def normalize_data_doi(self,mention: str) ->str:\n",
    "        \n",
    "        if not mention:\n",
    "            return \"\"\n",
    "        doi = mention.strip().replace('\\n', '').replace(' ', '')\n",
    "        doi = doi.replace('https://dx/','')\n",
    "        doi = re.sub(r'^(doi:|DOI:)', '', doi, flags=re.IGNORECASE)\n",
    "        doi = re.sub(r'^(https?://)?(dx\\.)?doi\\.org/', '', doi, flags=re.IGNORECASE)\n",
    "        doi = f\"https://doi.org/{doi.lower()}\" \n",
    "        doi = unicodedata.normalize('NFKD', doi)\n",
    "        \n",
    "        return doi\n",
    "        \n",
    "    def normalize_all_dois(self, dois_mentions):\n",
    "        return [\n",
    "            {**item, 'mention': self.normalize_data_doi(item['mention'])}\n",
    "            for item in dois_mentions\n",
    "        ]\n",
    "\n",
    "\n",
    "    def normalize_data_acc(self, mentions: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Normalize a list of extracted accession mentions.\n",
    "        Rules:\n",
    "            1. Normalize diacritics, casing, and whitespace\n",
    "            2. Support valid accession formats (GEO, EGA, GISAID, etc.)\n",
    "            3. If range detected, split with smart logic\n",
    "            4. For each accession, expand to all with same prefix in context\n",
    "            5. Remove duplicates\n",
    "\n",
    "        Args:\n",
    "            mentions (List[dict]): list of mention dicts with 'mention' key\n",
    "            context (str): full text context where mentions were extracted\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: normalized, expanded and deduplicated mentions\n",
    "        \"\"\"\n",
    "        def expand_mention_range(mention: str, context: str) -> list[str]:\n",
    "            mention = mention.strip()\n",
    "        \n",
    "            # Heuristic: determine the prefix by cutting at the last underscore (e.g., LT05_232072_)\n",
    "            sep_pos = mention.rfind('_')\n",
    "        \n",
    "            if sep_pos != -1:\n",
    "                prefix = mention[:sep_pos + 1]\n",
    "            else:\n",
    "                # Fallback: use leading alphabetical characters as prefix (e.g., \"GSE\" in GSE12345)\n",
    "                match = re.match(r'^([A-Z]+)', mention, flags=re.I)\n",
    "                prefix = match.group(1) if match else ''\n",
    "        \n",
    "            if not prefix:\n",
    "                return [mention]\n",
    "        \n",
    "            # Escape the prefix and build a regex to find all matching accessions with same prefix\n",
    "            prefix_escaped = re.escape(prefix)\n",
    "            # pattern = re.compile(rf'\\b{prefix_escaped}[0-9_.\\-]+\\b', re.IGNORECASE)\n",
    "            pattern = re.compile(rf'\\b{prefix_escaped}\\d{{2,}}[0-9_.\\-]*\\b', re.IGNORECASE)\n",
    "        \n",
    "            # Extract and deduplicate matching accessions from the context\n",
    "            found = set(m for m in pattern.findall(context))\n",
    "        \n",
    "            return sorted(found)\n",
    "\n",
    "        def split_range(norm: str) -> List[str]:\n",
    "            match = re.search(r'(.+?)\\s*-\\s*(.+)', norm)\n",
    "            if not match:\n",
    "                return [norm.strip()]\n",
    "            \n",
    "            left, right = match.group(1).strip(), match.group(2).strip()\n",
    "\n",
    "            left_clean = left.replace('.', '').replace(',', '').strip()\n",
    "            right_clean = right.replace('.', '').replace(',', '').strip()\n",
    "        \n",
    "            if left_clean.isdigit() and right_clean.isdigit():\n",
    "                return [left, right]\n",
    "        \n",
    "            left_alpha = re.match(r'^[A-Za-z]+', left)\n",
    "            right_alpha = re.match(r'^[A-Za-z]+', right)\n",
    "        \n",
    "            if left_alpha and right_alpha and left_alpha.group() == right_alpha.group():\n",
    "                return [left, right]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        pattern = re.compile(\n",
    "            r'^[A-Z0-9_.\\-]{4,}$'                      \n",
    "            r'|'\n",
    "            r'^[A-Z0-9_.\\-]{4,}\\s*-\\s*[A-Z0-9_.\\-]{4,}$' \n",
    "            ,\n",
    "            flags=re.IGNORECASE \n",
    "        )\n",
    "\n",
    "        cleaned_mentions = []\n",
    "        seen = set()\n",
    "\n",
    "        for mention_dict in mentions:\n",
    "            raw = mention_dict.get(\"mention\", \"\")\n",
    "            if not raw:\n",
    "                continue\n",
    "                \n",
    "            context = mention_dict.get(\"standard_context\", \"\") \n",
    "            if not context:\n",
    "                continue\n",
    "\n",
    "            # Normalize: remove diacritics, trim, uppercase\n",
    "            norm = unicodedata.normalize('NFKD', raw).strip().upper()\n",
    "\n",
    "            if not pattern.match(norm):\n",
    "                continue\n",
    "\n",
    "            # Handle ranges\n",
    "            if '-' in norm:\n",
    "                split_parts = split_range(norm)\n",
    "            else:\n",
    "                split_parts = [norm]\n",
    "\n",
    "            for part in split_parts:\n",
    "                part = part.strip()\n",
    "                # Expand mention range in context\n",
    "                expanded = expand_mention_range(part, context)\n",
    "                for acc in expanded:\n",
    "                    if acc not in seen:\n",
    "                        new_dict = mention_dict.copy()\n",
    "                        new_dict[\"mention\"] = acc\n",
    "                        cleaned_mentions.append(new_dict)\n",
    "                        seen.add(acc)\n",
    "\n",
    "        return cleaned_mentions\n",
    "\n",
    "    def deduplicate_extractions(self,extractions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Deduplicates mention dictionaries based on normalized mention string,\n",
    "        keeping the one with the highest confidence, regardless of type.\n",
    "        If multiple mentions have the same value and same type, the one with higher confidence is kept.\n",
    "        If types differ, the one with highest confidence is still preferred.\n",
    "    \n",
    "        :param extractions: List of mention dictionaries, each with keys:\n",
    "                            - \"mention\"\n",
    "                            - \"type\"\n",
    "                            - \"confidence\"\n",
    "        :return: Deduplicated list\n",
    "        \"\"\"\n",
    "        best_by_mention = {}\n",
    "    \n",
    "        for m in extractions:\n",
    "            key = m[\"mention\"]\n",
    "            new_conf = m.get(\"confidence\", 0)\n",
    "    \n",
    "            if key not in best_by_mention:\n",
    "                best_by_mention[key] = m\n",
    "            else:\n",
    "                existing = best_by_mention[key]\n",
    "                existing_conf = existing.get(\"confidence\", 0)\n",
    "    \n",
    "                # Always keep the one with the highest confidence\n",
    "                if new_conf > existing_conf:\n",
    "                    best_by_mention[key] = m\n",
    "    \n",
    "        return list(best_by_mention.values())\n",
    "\n",
    "        \n",
    "    def process_articles(\n",
    "        self,\n",
    "        output_csv: Path = SUBMISSION_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Main method to run the pipeline.\n",
    "\n",
    "        Args:\n",
    "            articles (Dict[str, str] or List[Path]): Raw texts or file paths.\n",
    "            file_format (str): Format of the articles (\"pdf\" or \"xml\").\n",
    "            output_csv (Path): Path to output CSV.\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        row_id = 0\n",
    "\n",
    "        raw_texts = self.text_dicts\n",
    "        pdf_texts = raw_texts.get(\"pdf\", {})\n",
    "        xml_texts = raw_texts.get(\"xml\", {})\n",
    "        \n",
    "        self.logger.info(f\"Extraction pipeline starting: {len(pdf_texts)} PDF articles to process ...\")\n",
    "        self.logger.info(f\"Extraction pipeline starting: {len(xml_texts)} XML articles to process ...\")\n",
    "        \n",
    "        # Extract, filter and classify data mentions\n",
    "        classified_mentions = self.mention_extractor_classifier.process_articles(raw_texts)\n",
    "        #print(classified_mentions)\n",
    "        # Add usage type labels, normalize DOIs if nedded and prepare data for CSV export\n",
    "        for article_id,  mentions in classified_mentions.items():\n",
    "            # Normalize the id of the article itself\n",
    "            article_id = self.normalize_article_doi(article_id)\n",
    "            # Remove articles with missing ids\n",
    "            if article_id in self.missing_ids:\n",
    "                continue \n",
    "            # Split mentions by format: DOI vs ACC\n",
    "            doi_mentions = [m for m in mentions if m[\"mention_format\"].upper() == \"DOI\"]\n",
    "            acc_mentions = [m for m in mentions if m[\"mention_format\"].upper() == \"ACC\"]\n",
    "            # Normalize DOI mentions\n",
    "            normalized_dois = self.normalize_all_dois(doi_mentions)\n",
    "            # Remove duplicated dois\n",
    "            normalized_dois = self.deduplicate_extractions(normalized_dois)\n",
    "            # Normalize ACC mentions\n",
    "            acc_mentions = self.normalize_data_acc(acc_mentions)\n",
    "            # Combine all the mentions back\n",
    "            mentions = normalized_dois + acc_mentions\n",
    "            \n",
    "            for mention in mentions:\n",
    "                \n",
    "                mention_text = mention[\"mention\"]\n",
    "               \n",
    "                type_value = mention.get(\"type\", 2)\n",
    "                \n",
    "                if type_value not in [0, 1]:\n",
    "                    \n",
    "                    continue  # Skip mention with unknown classification\n",
    "                \n",
    "                usage_type = \"Primary\" if type_value == 0 else \"Secondary\"\n",
    "\n",
    "                results.append({\n",
    "                    \"row_id\": row_id,\n",
    "                    \"article_id\": article_id,\n",
    "                    \"dataset_id\": mention_text,\n",
    "                    \"type\": usage_type\n",
    "                })\n",
    "                row_id += 1\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Drop duplicates based on article_id + dataset_id\n",
    "        df[\"article_id\"] = df[\"article_id\"].astype(str)\n",
    "        df[\"dataset_id\"] = df[\"dataset_id\"].astype(str)\n",
    "        df[\"type\"] = df[\"type\"].astype(str)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"article_id\", \"dataset_id\"], inplace=True)\n",
    "        \n",
    "        # Drop rows with missing values in essential columns\n",
    "        df.dropna(subset=[\"article_id\", \"dataset_id\", \"type\"], inplace=True)\n",
    "        \n",
    "        #  Drop the old provisional row_id (if it exists)\n",
    "        if \"row_id\" in df.columns:\n",
    "            df.drop(columns=[\"row_id\"], inplace=True)\n",
    "        \n",
    "        # Reindex and create a clean row_id\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.insert(0, \"row_id\", df.index)\n",
    "\n",
    "        try:\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            self.logger.info(f\"Extraction pipeline complete. Results saved to: {output_csv}\")\n",
    "            print(f\"Print fallback: Results saved to: {output_csv}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save CSV: {e}\")\n",
    "            print(f\"Failed to write output to {output_csv}: {e}\")\n",
    "\n",
    "\n",
    "        self.logger.info(f\"{len(df)} rows written to submission file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:10:33.850557Z",
     "iopub.status.busy": "2025-09-09T19:10:33.850257Z",
     "iopub.status.idle": "2025-09-09T19:10:33.871156Z",
     "shell.execute_reply": "2025-09-09T19:10:33.870253Z",
     "shell.execute_reply.started": "2025-09-09T19:10:33.850526Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_labels_df = train_labels_df[train_labels_df.type=='Missing']\n",
    "missing_ids = [str(aid) for aid in missing_labels_df.article_id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:10:33.872364Z",
     "iopub.status.busy": "2025-09-09T19:10:33.872113Z",
     "iopub.status.idle": "2025-09-09T19:10:34.483593Z",
     "shell.execute_reply": "2025-09-09T19:10:34.482759Z",
     "shell.execute_reply.started": "2025-09-09T19:10:33.87234Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_data_directory(data_root: Path, subdir: str, ext: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Attempt to find a directory containing files with given extension.\n",
    "    \n",
    "    Checks in this order:\n",
    "    1. Preferred directory: `data_root / subdir`\n",
    "    2. 'test' directory itself: `data_root / 'test'`\n",
    "    3. Any subdirectory inside 'test' that contains matching files\n",
    "    \n",
    "    Returns:\n",
    "        Path to directory containing the files, or None if no files found.\n",
    "    \"\"\"\n",
    "    preferred_dir = data_root / subdir\n",
    "    test_dir = data_root / \"test\"\n",
    "\n",
    "    # Check preferred directory first\n",
    "    if preferred_dir.exists() and preferred_dir.is_dir():\n",
    "        if any(preferred_dir.glob(f\"*.{ext}\")):\n",
    "            return preferred_dir\n",
    "\n",
    "    # Check 'test' directory itself\n",
    "    if test_dir.exists() and any(test_dir.glob(f\"*.{ext}\")):\n",
    "        return test_dir\n",
    "\n",
    "    # Check subdirectories under 'test'\n",
    "    if test_dir.exists():\n",
    "        for sub in test_dir.iterdir():\n",
    "            if sub.is_dir() and any(sub.glob(f\"*.{ext}\")):\n",
    "                return sub\n",
    "\n",
    "    # No directory with matching files found\n",
    "    return None\n",
    "\n",
    "def get_test_files(data_root: Path, subdir: str, ext: str) -> Tuple[List[Path], Optional[Path]]:\n",
    "    \"\"\"\n",
    "    Locate the directory containing files with given extension and return the file paths.\n",
    "\n",
    "    Returns:\n",
    "        - List of Path objects for files with given extension\n",
    "        - The Path to the directory containing these files, or None if none found\n",
    "    \"\"\"\n",
    "    dir_path = find_data_directory(data_root, subdir, ext)\n",
    "    if dir_path is None:\n",
    "        return [], None\n",
    "\n",
    "    files = list(dir_path.glob(f\"*.{ext}\"))\n",
    "    if not files:\n",
    "        return [], None\n",
    "\n",
    "    print(f\"{len(files)} {ext.upper()} files found in {dir_path} for testing.\")\n",
    "    return files, dir_path\n",
    "\n",
    "\n",
    "\n",
    "# Attempt to find PDF and XML test files\n",
    "test_pdf_paths, TEST_PDF_DIR = get_test_files(DATA_ROOT, \"test/PDF\", \"pdf\")\n",
    "test_xml_paths, TEST_XML_DIR = get_test_files(DATA_ROOT, \"test/XML\", \"xml\")\n",
    "\n",
    "\n",
    "# Filter out missing IDs immediately\n",
    "test_pdf_paths = [p for p in test_pdf_paths if p.stem not in missing_ids]\n",
    "test_xml_paths = [p for p in test_xml_paths if p.stem not in missing_ids]\n",
    "\n",
    "\n",
    "test_text_dicts = {\"pdf\": {}, \"xml\": {}}\n",
    "\n",
    "pdf_text_extractor = ExtractTextFromArticles(file_format=\"PDF\")\n",
    "xml_text_extractor = ExtractTextFromArticles(file_format=\"XML\")\n",
    "\n",
    "# Process whichever test data is available\n",
    "if test_pdf_paths and test_xml_paths:\n",
    "    test_pdf_only_paths = get_pdf_only(test_pdf_paths,test_xml_paths)\n",
    "    \n",
    "    pdf_test_texts = pdf_text_extractor.extract_text_data(test_pdf_only_paths)\n",
    "    pdf_clean_test_texts = pdf_text_extractor.clean_articles(pdf_test_texts)\n",
    "\n",
    "    xml_test_texts = xml_text_extractor.extract_text_data(test_xml_paths)\n",
    "    xml_clean_test_texts = xml_text_extractor.clean_articles(xml_test_texts)\n",
    "\n",
    "    test_text_dicts['pdf'] = pdf_clean_test_texts\n",
    "    test_text_dicts['xml'] = xml_clean_test_texts\n",
    "    \n",
    "elif test_pdf_paths:\n",
    "    pdf_test_texts = pdf_text_extractor.extract_text_data(test_pdf_paths)\n",
    "    pdf_clean_test_texts = pdf_text_extractor.clean_articles(pdf_test_texts)\n",
    "\n",
    "    test_text_dicts['pdf'] = pdf_clean_test_texts\n",
    "\n",
    "elif test_xml_paths:\n",
    "    xml_test_texts = xml_text_extractor.extract_text_data(test_xml_paths)\n",
    "    xml_clean_test_texts = xml_text_extractor.clean_articles(xml_test_texts)\n",
    "\n",
    "    test_text_dicts['xml'] = xml_clean_test_texts\n",
    "\n",
    "else:\n",
    "    # Neither PDF nor XML test files found - raise a clear error\n",
    "    raise RuntimeError(\n",
    "        \"No test data found: no PDF files under 'test/PDF' or 'test' directories, \"\n",
    "        \"and no XML files under 'test/XML' or 'test' directories.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:10:34.484663Z",
     "iopub.status.busy": "2025-09-09T19:10:34.484452Z",
     "iopub.status.idle": "2025-09-09T19:11:28.060591Z",
     "shell.execute_reply": "2025-09-09T19:11:28.060022Z",
     "shell.execute_reply.started": "2025-09-09T19:10:34.484646Z"
    }
   },
   "outputs": [],
   "source": [
    "mention_filter = DatasetMentionFilter(confidence_threshold=0.70)\n",
    "cls_preparator = ClassificationDataPreparator(tokenizer=cls_tokenizer)\n",
    "\n",
    "mention_extractor = MentionExtractorAndClassifier(\n",
    "    ner_model=ner_model,\n",
    "    ner_tokenizer=ner_tokenizer,\n",
    "    classifier_model=cls_model,\n",
    "    classifier_tokenizer=cls_tokenizer,\n",
    "    mention_filter=mention_filter,\n",
    "    cls_preparator=cls_preparator,\n",
    "    classification_window=300,\n",
    "    strides=[256, 312,412, 484],\n",
    "    text_dicts = test_text_dicts\n",
    ")\n",
    "\n",
    "data_references_finder = FindDataReferences(mention_extractor,mention_filter,\n",
    "                                            test_text_dicts,missing_ids)\n",
    "\n",
    "data_references_finder.process_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:11:28.061566Z",
     "iopub.status.busy": "2025-09-09T19:11:28.061337Z",
     "iopub.status.idle": "2025-09-09T19:11:28.066832Z",
     "shell.execute_reply": "2025-09-09T19:11:28.066075Z",
     "shell.execute_reply.started": "2025-09-09T19:11:28.061548Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(SUBMISSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T19:11:28.068162Z",
     "iopub.status.busy": "2025-09-09T19:11:28.067615Z",
     "iopub.status.idle": "2025-09-09T19:11:28.093982Z",
     "shell.execute_reply": "2025-09-09T19:11:28.09332Z",
     "shell.execute_reply.started": "2025-09-09T19:11:28.068143Z"
    }
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "datasetId": 7960664,
     "sourceId": 12603186,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 411028,
     "modelInstanceId": 392355,
     "sourceId": 493742,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
