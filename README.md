# ğŸ§  CompÃ©tition Kaggle : Make Data Count - Finding Data References

## ğŸš€ Vue d'ensemble du Projet

Ce dÃ©pÃ´t contient le code et l'approche dÃ©taillÃ©e soumis Ã  la compÃ©tition Kaggle **"Make Data Count - Finding Data References"**.  
L'objectif Ã©tait d'identifier, d'extraire et de classer les mentions de jeux de donnÃ©es (*Data References*) au sein d'articles scientifiques, distinguant leur type d'utilisation :  

- **Primaire** (gÃ©nÃ©ration du jeu de donnÃ©es);
- **Secondaire** (rÃ©utilisation de donnÃ©es existantes).  

Notre solution repose sur un **pipeline d'apprentissage automatique en deux Ã©tapes** (**NER + Classification**) combinÃ© Ã  des techniques de **post-traitement heuristique** afin de garantir la robustesse et la prÃ©cision des identifiants extraits (DOI et Accession IDs).

---

## ğŸ’¡ Approche Technique

Le pipeline de solution est structurÃ© en plusieurs Ã©tapes critiques :

### 1. PrÃ©-traitement et Nettoyage des DonnÃ©es (Data Cleaning)

**Extraction de Texte Robuste :**  
Utilisation de **PyMuPDF (fitz)** pour extraire le texte brut Ã  partir des fichiers PDF, avec une logique de nettoyage pour supprimer les en-tÃªtes et pieds de page rÃ©pÃ©titifs.  
Le texte des fichiers XML a Ã©tÃ© extrait Ã  lâ€™aide de `xml.etree.ElementTree`.

**Nettoyage des Labels dâ€™EntraÃ®nement :**  
Correction manuelle et automatique des mentions de jeux de donnÃ©es dans `train_labels.csv` qui Ã©taient tronquÃ©es, mal formatÃ©es ou incohÃ©rentes avec le texte source (â‰ˆ 50 cas identifiÃ©s et corrigÃ©s).

**Standardisation des DOI :**  
Normalisation de tous les identifiants vers un format uniforme :  
`https://doi.org/10.xxxx/...`

---

### 2. ModÃ©lisation : Extraction dâ€™EntitÃ©s NommÃ©es (NER)

La premiÃ¨re Ã©tape consiste Ã  localiser toutes les mentions de DOI et dâ€™Accession IDs dans le texte.

- **ModÃ¨le :** `allenai/scibert_scivocab_uncased` (fine-tunÃ©)  
- **TÃ¢che :** Token Classification (Ã©tiquetage BIO : `B-DOI`, `I-DOI`, `B-ACC`, `I-ACC`)  
- **Technique :** FenÃªtre glissante (*Sliding Window*) avec chevauchement pour gÃ©rer les articles longs (max 512 tokens)  
- **Ã‰valuation :** Le modÃ¨le NER a atteint un F1-score Ã©levÃ© sur lâ€™ensemble de validation.

---

### 3. Post-traitement et Filtrage Heuristique

AprÃ¨s l'extraction NER, un ensemble de rÃ¨gles a Ã©tÃ© appliquÃ© pour filtrer les faux positifs et amÃ©liorer la qualitÃ© :

- **Filtrage des Faux Positifs :**  
  Exclusion des mentions liÃ©es aux DOIs dâ€™articles scientifiques ou Ã  des numÃ©ros de financement, via des regex et mots-clÃ©s contextuels (`funding`, `grant`, etc.).  
- **Filtrage des Mentions TronquÃ©es/Invalides :**  
  Suppression des identifiants trop courts ou ne respectant pas les formats DOI/Accession.  
- **Expansion des Accession IDs :**  
  Gestion des plages dâ€™identifiants (ex : `GSE12345-GSE12350`) pour identifier automatiquement les entrÃ©es associÃ©es.

---

### 4. ModÃ©lisation : Classification de lâ€™Usage (Primaire vs Secondaire)

La seconde Ã©tape consiste Ã  **classer le type dâ€™usage** des jeux de donnÃ©es extraits.

- **ModÃ¨le :** `allenai/scibert_scivocab_uncased` (fine-tunÃ©)  
- **TÃ¢che :** Sequence Classification (`Primary` ou `Secondary`)  
- **Contexte Enrichi :** Extension du contexte textuel au paragraphe complet autour de la mention, en particulier les sections *Data Availability*.  
- **StratÃ©gie de Split :** `StratifiedGroupKFold` garantissant quâ€™un mÃªme article ne figure pas Ã  la fois dans lâ€™entraÃ®nement et la validation (prÃ©vention du data leakage).

---

## ğŸ› ï¸ DÃ©pendances

Le projet a Ã©tÃ© dÃ©veloppÃ© en **Python 3.x** et nÃ©cessite les bibliothÃ¨ques principales suivantes :

| BibliothÃ¨que | Description |
|---------------|-------------|
| `transformers` | ModÃ¨les SciBERT pour NER et Classification |
| `datasets` | Gestion des jeux de donnÃ©es Hugging Face |
| `scikit-learn` | MÃ©triques et validation croisÃ©e (GroupShuffleSplit / StratifiedGroupKFold) |
| `pandas`, `numpy` | Manipulation et prÃ©paration des donnÃ©es |
| `pymupdf` (fitz) | Extraction de texte Ã  partir des PDF |
| `seqeval` | MÃ©triques NER (F1-score, prÃ©cision, rappel) |
| `torch` | Backend d'entraÃ®nement (CUDA/GPU requis) |

---

## ğŸ“‚ Structure du Code

Le code complet est contenu dans le notebook :  
`final-notebook-for-mdc-challenge.ipynb`

**Classes principales :**

| Classe | Description |
|---------|-------------|
| `ExtractTextFromArticles` | Extraction de texte propre Ã  partir des fichiers PDF/XML |
| `DOIsFormatHandler` | Nettoyage, normalisation et validation des DOIs extraits |
| `TrainingDataPreparator` | PrÃ©paration et Ã©tiquetage BIO pour NER + contexte pour classification |
| `DatasetMentionFilter` | Application des rÃ¨gles heuristiques de filtrage et validation |
| `MentionExtractorAndClassifier` | Pipeline dâ€™infÃ©rence combinÃ©e NER + classification |
| `FindDataReferences` | Pipeline complet orchestrant extraction, classification et gÃ©nÃ©ration de soumission |

---

## ğŸ“ˆ RÃ©sultats et Performances

### ModÃ¨le NER  

(F1-score micro sur lâ€™ensemble de validation, hors classe `O`)

| EntitÃ© | PrÃ©cision | Rappel | F1-Score |
|---------|------------|---------|----------|
| DOI | 0.91 | 0.88 | 0.90 |
| ACC | 0.93 | 0.91 | 0.92 |
| **Global** | **0.92** | **0.89** | **0.91** |

### ModÃ¨le de Classification  

(F1-score pondÃ©rÃ© sur lâ€™ensemble de validation)

| MÃ©trique | Valeur |
|-----------|---------|
| Accuracy | 0.87 |
| F1-Score (Weighted) | 0.87 |

> Voir la matrice de confusion dans le notebook pour la rÃ©partition des classes.

---

## ğŸ”— Lien vers la CompÃ©tition

ğŸ‘‰ [Make Data Count - Finding Data References (Kaggle)](https://www.kaggle.com/competitions/make-data-count-finding-data-references)
