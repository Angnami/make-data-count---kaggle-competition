# üß† Comp√©tition Kaggle : Make Data Count - Finding Data References

## üöÄ Vue d'ensemble du Projet

Ce d√©p√¥t contient le code et l'approche d√©taill√©e soumis √† la comp√©tition Kaggle **"Make Data Count - Finding Data References"**.  
L'objectif √©tait d'identifier, d'extraire et de classer les mentions de jeux de donn√©es (*Data References*) au sein d'articles scientifiques, distinguant leur type d'utilisation :  

- **Primaire** (g√©n√©ration du jeu de donn√©es);
- **Secondaire** (r√©utilisation de donn√©es existantes).  

Notre solution repose sur un **pipeline d'apprentissage automatique en deux √©tapes** (**NER + Classification**) combin√© √† des techniques de **post-traitement heuristique** afin de garantir la robustesse et la pr√©cision des identifiants extraits (DOI et Accession IDs).

---

## üí° Approche Technique

Le pipeline de solution est structur√© en plusieurs √©tapes critiques :

### 1. Pr√©-traitement et Nettoyage des Donn√©es (Data Cleaning)

**Extraction de Texte Robuste :**  
Utilisation de **PyMuPDF (fitz)** pour extraire le texte brut √† partir des fichiers PDF, avec une logique de nettoyage pour supprimer les en-t√™tes et pieds de page r√©p√©titifs.  
Le texte des fichiers XML a √©t√© extrait √† l‚Äôaide de `xml.etree.ElementTree`.

**Nettoyage des Labels d‚ÄôEntra√Ænement :**  
Correction manuelle et automatique des mentions de jeux de donn√©es dans `train_labels.csv` qui √©taient tronqu√©es, mal format√©es ou incoh√©rentes avec le texte source (‚âà 50 cas identifi√©s et corrig√©s).

**Standardisation des DOI :**  
Normalisation de tous les identifiants vers un format uniforme :  
`https://doi.org/10.xxxx/...`

---

### 2. Mod√©lisation : Extraction d‚ÄôEntit√©s Nomm√©es (NER)

La premi√®re √©tape consiste √† localiser toutes les mentions de DOI et d‚ÄôAccession IDs dans le texte.

- **Mod√®le :** `allenai/scibert_scivocab_uncased` (fine-tun√©)  
- **T√¢che :** Token Classification (√©tiquetage BIO : `B-DOI`, `I-DOI`, `B-ACC`, `I-ACC`)  
- **Technique :** Fen√™tre glissante (*Sliding Window*) avec chevauchement pour g√©rer les articles longs (max 512 tokens)  
- **√âvaluation :** Le mod√®le NER a atteint un F1-score √©lev√© sur l‚Äôensemble de validation.

---

### 3. Post-traitement et Filtrage Heuristique

Apr√®s l'extraction NER, un ensemble de r√®gles a √©t√© appliqu√© pour filtrer les faux positifs et am√©liorer la qualit√© :

- **Filtrage des Faux Positifs :**  
  Exclusion des mentions li√©es aux DOIs d‚Äôarticles scientifiques ou √† des num√©ros de financement, via des regex et mots-cl√©s contextuels (`funding`, `grant`, etc.).  
- **Filtrage des Mentions Tronqu√©es/Invalides :**  
  Suppression des identifiants trop courts ou ne respectant pas les formats DOI/Accession.  
- **Expansion des Accession IDs :**  
  Gestion des plages d‚Äôidentifiants (ex : `GSE12345-GSE12350`) pour identifier automatiquement les entr√©es associ√©es.

---

### 4. Mod√©lisation : Classification de l‚ÄôUsage (Primaire vs Secondaire)

La seconde √©tape consiste √† **classer le type d‚Äôusage** des jeux de donn√©es extraits.

- **Mod√®le :** `allenai/scibert_scivocab_uncased` (fine-tun√©)  
- **T√¢che :** Sequence Classification (`Primary` ou `Secondary`)  
- **Contexte Enrichi :** Extension du contexte textuel au paragraphe complet autour de la mention, en particulier les sections *Data Availability*.  
- **Strat√©gie de Split :** `StratifiedGroupKFold` garantissant qu‚Äôun m√™me article ne figure pas √† la fois dans l‚Äôentra√Ænement et la validation (pr√©vention du data leakage).

---

## üõ†Ô∏è D√©pendances

Le projet a √©t√© d√©velopp√© en **Python 3.x** et n√©cessite les biblioth√®ques principales suivantes :

| Biblioth√®que | Description |
|---------------|-------------|
| `transformers` | Mod√®les SciBERT pour NER et Classification |
| `datasets` | Gestion des jeux de donn√©es Hugging Face |
| `scikit-learn` | M√©triques et validation crois√©e (GroupShuffleSplit / StratifiedGroupKFold) |
| `pandas`, `numpy` | Manipulation et pr√©paration des donn√©es |
| `pymupdf` (fitz) | Extraction de texte √† partir des PDF |
| `seqeval` | M√©triques NER (F1-score, pr√©cision, rappel) |
| `torch` | Backend d'entra√Ænement (CUDA/GPU requis) |

---

## üìÇ Structure du Code

Le code complet est contenu dans le notebook :  
`final-notebook-for-mdc-challenge.ipynb`

**Classes principales :**

| Classe | Description |
|---------|-------------|
| `ExtractTextFromArticles` | Extraction de texte propre √† partir des fichiers PDF/XML |
| `DOIsFormatHandler` | Nettoyage, normalisation et validation des DOIs extraits |
| `TrainingDataPreparator` | Pr√©paration et √©tiquetage BIO pour NER + contexte pour classification |
| `DatasetMentionFilter` | Application des r√®gles heuristiques de filtrage et validation |
| `MentionExtractorAndClassifier` | Pipeline d‚Äôinf√©rence combin√©e NER + classification |
| `FindDataReferences` | Pipeline complet orchestrant extraction, classification et g√©n√©ration de soumission |

---

## üìà R√©sultats et Performances

### Mod√®le NER  

(F1-score micro sur l‚Äôensemble de validation, hors classe `O`)

| Entit√© | Pr√©cision | Rappel | F1-Score |
|---------|------------|---------|----------|
| DOI | 0.91 | 0.88 | 0.90 |
| ACC | 0.93 | 0.91 | 0.92 |
| **Global** | **0.92** | **0.89** | **0.91** |

### Mod√®le de Classification  

(F1-score pond√©r√© sur l‚Äôensemble de validation)

| M√©trique | Valeur |
|-----------|---------|
| Accuracy | 0.87 |
| F1-Score (Weighted) | 0.87 |

---

## üîó Lien vers la Comp√©tition

üëâ [Make Data Count - Finding Data References (Kaggle)](https://www.kaggle.com/competitions/make-data-count-finding-data-references)
